{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNqpqVs9g3kA"
      },
      "source": [
        "#Import cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJyZR9Q351CE"
      },
      "outputs": [],
      "source": [
        "! mkdir /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkYja-9MWqdX"
      },
      "outputs": [],
      "source": [
        "! wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=d.a.vandendoel@students.uu.nl&password=Derkojo95!&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1 -P /content/cityScapes\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3 -P /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ps2MJcC6K49"
      },
      "outputs": [],
      "source": [
        "! unzip /content/cityScapes/gtFine_trainvaltest.zip -d /content/cityScapes/annotations\n",
        "! unzip /content/cityScapes/leftImg8bit_trainvaltest.zip -d /content/cityScapes/img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGT_vwW0hJRK"
      },
      "source": [
        "#Mount drive and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waujOpX8RF0k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufsj9SjJsEFT"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install cityscapesscripts"
      ],
      "metadata": {
        "id": "BC3rkgDtoq6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Scriptie/python\n",
        "import Generators\n",
        "import Discriminators"
      ],
      "metadata": {
        "id": "CVqeiaDaCD_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIJTdDilt6-Y"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "import cv2\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "import PIL\n",
        "import scipy\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "\n",
        "from cityscapesscripts.preparation.json2instanceImg import json2instanceImg\n",
        "from cityscapesscripts.preparation.json2labelImg import json2labelImg\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import SpectralNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "from tqdm import tqdm\n",
        "from random import randint, seed\n",
        "from google.colab.patches import cv2_imshow\n",
        "from imageio import imread\n",
        "from tensorflow.keras import layers\n",
        "from glob import glob\n",
        "from IPython import display\n",
        "from random import randint\n",
        "from skimage.metrics import structural_similarity as SSIM\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "print(os.listdir(\"../content/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load images"
      ],
      "metadata": {
        "id": "MM_O9x7oIP5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IM_HEIGHT = 256\n",
        "IM_WIDTH = 512\n",
        "BUFFER_SIZE = 2975\n",
        "BATCH_SIZE = 1\n",
        "OUTPUT_CHANNELS = 3\n",
        "MASK_HEIGTH = 64\n",
        "MASK_WIDTH = 32\n",
        "LAMBDA = 100"
      ],
      "metadata": {
        "id": "IGDdQvdAIPTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(image_file, channels):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.io.decode_png(image, channels = channels)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "CD51LwvyIPV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(input_image, height, width):\n",
        "    image = tf.image.resize(\n",
        "        input_image, \n",
        "        [height, width],\n",
        "        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
        "    )\n",
        "  \n",
        "    return image"
      ],
      "metadata": {
        "id": "MDDXJGnQIUz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize values between -1 and +1\n",
        "def normalize(image):\n",
        "  image = image / 127.5 - 1\n",
        "  return image\n",
        "\n",
        "# Normalize values between 0 and 1 for image display\n",
        "def denormalize(image):\n",
        "  return image *0.5+0.5"
      ],
      "metadata": {
        "id": "EgJfltZwIVvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training dataset"
      ],
      "metadata": {
        "id": "PNlSKlBUKlbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill pandas dataframe with the paths to the files\n",
        "def folder_to_pd(image_dir, semantics_dir):\n",
        "  image_paths = {\"train\": [], \"val\": []}\n",
        "  semantics_paths = {\"train\": [], \"val\": []}\n",
        "  label_paths = {\"train\": [], \"val\": []}\n",
        "\n",
        "\n",
        "  for data_type in [\"train\", \"val\"]:\n",
        "    image_subdir = os.path.join(image_dir, data_type)\n",
        "    for root, subdir, files in os.walk(image_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for file in files:\n",
        "          image_paths[data_type].append(os.path.join(root, file))\n",
        "\n",
        "    semantics_subdir = os.path.join(semantics_dir, data_type)\n",
        "    for root, subdir, files in os.walk(semantics_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for i,file in enumerate(files):\n",
        "          if file[-12:] == \"labelIds.png\":\n",
        "            semantics_paths[data_type].append(os.path.join(root, file))\n",
        "          if file[-5:] == \".json\":\n",
        "            label_paths[data_type].append(os.path.join(root, file))\n",
        "  \n",
        "  train_ds = pd.DataFrame(list(zip(image_paths[\"train\"], semantics_paths[\"train\"], label_paths[\"train\"])), columns =['image', 'labelIds', 'json'])\n",
        "  val_ds = pd.DataFrame(list(zip(image_paths[\"val\"], semantics_paths[\"val\"], label_paths[\"val\"])), columns =['image', 'labelIds', 'json'])\n",
        "\n",
        "  return train_ds, val_ds\n",
        "\n",
        "#Function that determines whether there is a person present in the image\n",
        "def flag_person(json_path):\n",
        "  with open(json_path) as json_file:\n",
        "    json_object = json.load(json_file)\n",
        "  return any(element[\"label\"] == \"person\" for element in json_object[\"objects\"])\n",
        "\n",
        "#function that filters out all the images without images in them\n",
        "# if filter persons is true, get all images WITHOUT person. If False get all images WITH persons\n",
        "def filter_ds(ds, filter_person = True):\n",
        "  json_list = ds[\"json\"]\n",
        "  drop_rows = [i for i,element in enumerate(json_list) if flag_person(element) == filter_person]\n",
        "  return ds.drop(drop_rows)\n",
        "\n",
        "# convert pandas dataframe to tensorflow dataset\n",
        "def pd_to_ds(pd_df):\n",
        "  return tf.data.Dataset.from_tensor_slices((pd_df[\"image\"], pd_df[\"labelIds\"], pd_df[\"json\"])) #returns dataset with image and labels for each element in the dataset"
      ],
      "metadata": {
        "id": "52spedpIKnso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_CS(info, mask):\n",
        "  mask = load(mask, 0)\n",
        "  mask = mask /255\n",
        "\n",
        "  image_file, labels_file, json_file = info\n",
        "  image = load(image_file, 3)\n",
        "  image = resize(image, 256,512)\n",
        "  masked_image = image * (1-mask)\n",
        "  image = normalize(image)\n",
        "  masked_image = normalize(masked_image)  \n",
        "\n",
        "  labels = load(labels_file, 0)\n",
        "  labels = resize(labels, 256,512)\n",
        "  masked_labels = labels * (1-mask)\n",
        "\n",
        "  labels = tf.cast(labels, tf.int32)\n",
        "  masked_labels = tf.cast(masked_labels, tf.int32)\n",
        "\n",
        "  #image = label_to_rgb(tf.expand_dims(labels, 0))\n",
        "  #masked_image = label_to_rgb(tf.expand_dims(masked_labels, 0))\n",
        "\n",
        "  #image = tf.squeeze(image, 0)\n",
        "  #masked_image = tf.squeeze(masked_image, 0)\n",
        "\n",
        "  return image, masked_image, labels, masked_labels, mask"
      ],
      "metadata": {
        "id": "S-N48fnXeL-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_person(image_file, labels_file, json_file):\n",
        "  personID = 24\n",
        "  riderID = 25\n",
        "  \n",
        "  labels = load(labels_file, 1)\n",
        "  labels = resize(labels, 256,512)\n",
        "  labels = tf.cast(labels, tf.int32)\n",
        "    \n",
        "  mask_person = tf.math.equal(labels, personID)\n",
        "  mask_rider = tf.math.equal(labels, riderID)\n",
        "  mask_person = tf.cast(mask_person, tf.int32)\n",
        "  mask_rider = tf.cast(mask_rider, tf.int32)\n",
        "  mask = mask_person + mask_rider\n",
        "  masked_labels = labels * (1-mask)\n",
        "  \n",
        "  image = load(image_file, 3)\n",
        "  image = resize(image, 256,512)\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  masked_image = image * (1-mask)\n",
        "  image = normalize(image)\n",
        "  masked_image = normalize(masked_image)\n",
        "  \n",
        "  return image, masked_image, labels, masked_labels, mask"
      ],
      "metadata": {
        "id": "ZKOqG8UtF8I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create fine tuning datasets\n",
        "tf_train, tf_val = folder_to_pd(\"/content/cityScapes/img/leftImg8bit\", \"/content/cityScapes/annotations/gtFine\")"
      ],
      "metadata": {
        "id": "6xlVtrV1ViS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset with where each element is a tuple (path_to_image, path_to_labels) \n",
        "train_noPersons = pd_to_ds(filter_ds(tf_train))\n",
        "val_noPersons = pd_to_ds(filter_ds(tf_val))\n",
        "\n",
        "train_persons = pd_to_ds(filter_ds(tf_train, False))\n",
        "val_persons = pd_to_ds(filter_ds(tf_val, False))\n",
        "\n",
        "print(\"Training dataset WITHOUT persons has\", train_noPersons.cardinality().numpy(), \"elements.\")\n",
        "print(\"Validation dataset WITHOUT persons has\", val_noPersons.cardinality().numpy(), \"elements.\")\n",
        "print(\"\")\n",
        "print(\"Training dataset WITH persons has\", train_persons.cardinality().numpy(), \"elements.\")\n",
        "print(\"Validation dataset WITH persons has\", val_persons.cardinality().numpy(), \"elements.\")"
      ],
      "metadata": {
        "id": "lpLXqdQYXDAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip mask and image data together."
      ],
      "metadata": {
        "id": "bBzuQup9eZgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masks = tf.data.Dataset.list_files(\"/content/drive/MyDrive/Scriptie/masks/train/*\")\n",
        "masks_val = tf.data.Dataset.list_files(\"/content/drive/MyDrive/Scriptie/masks/val/*\")\n",
        "\n",
        "train_noPersons = tf.data.Dataset.zip((train_noPersons, masks))\n",
        "val_noPersons = tf.data.Dataset.zip((val_noPersons, masks_val))"
      ],
      "metadata": {
        "id": "CHwckUJCd6yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_train = train_noPersons.shuffle(train_noPersons.cardinality(), reshuffle_each_iteration=True)\n",
        "finetune_train = finetune_train.map(load_images_CS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "finetune_train = finetune_train.batch(BATCH_SIZE)\n",
        "#finetune_train = finetune_train.cache(\"/content/temporary.tfcache\")\n",
        "finetune_train = finetune_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "finetune_val = val_noPersons.map(load_images_CS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "finetune_val = finetune_val.batch(BATCH_SIZE)\n",
        "finetune_val = finetune_val.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WRl6tZ0NdZYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_val_persons = val_persons.map(load_images_person, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_val_persons = tf_val_persons.batch(BATCH_SIZE)\n",
        "tf_val_persons = tf_val_persons.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "d5-nfywGFnIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (image, masked_image, labels, masked_labels, mask) in enumerate(tf_val_persons):\n",
        "  if step == 396:\n",
        "    plt.imshow(masked_image[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "9QIISpNpn6iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up9HfI1T9iUP"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX2Jl_9mBOy3"
      },
      "outputs": [],
      "source": [
        "generator = Generators.Generator(3)\n",
        "discriminator = Discriminators.Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss( disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # mean absolute error\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "cTSsCGeQe4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "sTFne02efJiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the optimzer for the task of fine-tuning to get the best samples."
      ],
      "metadata": {
        "id": "U53HKKD0f3Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)"
      ],
      "metadata": {
        "id": "I265uMWXfEFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Scriptie/ckpt/finetune/GAN'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_1e-4\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "zkQTKAEbfGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(\"/content/drive/MyDrive/Scriptie/ckpt/GAN/ckpt_2e-5-1\")"
      ],
      "metadata": {
        "id": "A9t9DcD95ofU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, test_input, tar, mask):  \n",
        "  prediction = model(test_input, training=True)\n",
        "  prediction = prediction*mask+tar*(1-mask)\n",
        "  plt.figure(figsize=(20, 20))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(denormalize(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7V0Y6CutfI-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epoch(input_image, mask, target):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLoss(input_image, mask, target): \n",
        "  gen_output = generator(input_image, training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminator([input_image, target], training=True)\n",
        "  disc_generated_output = discriminator([input_image, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss"
      ],
      "metadata": {
        "id": "kNbg_FTqfQ88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/finetune/GAN/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_TrainFT\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_ValFT\")"
      ],
      "metadata": {
        "id": "FV9dSWdkfRTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_ds, test_ds, epochs):\n",
        "  example_target, example_input, label_test, label_masked_test, mask_test = next(iter(test_ds.take(1)))\n",
        "  example_input, example_target, mask_test\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    generate_images(generator, example_input, example_target, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(target, input_image, labels, masked_labels, mask) in train_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epoch(input_image, mask, target)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "\n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict[\"gen_L1_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "\n",
        "    \n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(target, input_image, labels, masked_labels, mask) in test_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = getValAccLoss(input_image, mask, target)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict_val[\"gen_L1_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "#    if epoch % 5 == 0 & epoch !=0: \n",
        "#      checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "O83MycNUfRWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(finetune_train, finetune_val, 5)"
      ],
      "metadata": {
        "id": "ZVzbU-0K5zTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.save(\"/content/drive/MyDrive/Scriptie/ckpt/finetune/GAN/GAN\")"
      ],
      "metadata": {
        "id": "L9327qcM-3b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training SemGAN"
      ],
      "metadata": {
        "id": "1q_3628CUNR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SemanticsGAN"
      ],
      "metadata": {
        "id": "CLUPcnoecq5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (246, 35,232) ), #244,35,232\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ), #107 142 35\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]\n",
        "\n",
        "#convert rgb to grayscale values, this produces a unique grayscale value for each label\n",
        "def rgb_gray(rgb):\n",
        "  r, g ,b = rgb\n",
        "  return 0.2989*r + 0.5870*g + 0.1140*b\n",
        "\n",
        "#create dictionary with grayscale as key and id as value\n",
        "color_id_list = []\n",
        "color_id = {}\n",
        "id_r = {}\n",
        "id_g = {}\n",
        "id_b = {}\n",
        "for label in labels:\n",
        "  key_list = tf.image.rgb_to_grayscale(tf.convert_to_tensor(label.color))\n",
        "  key = int(rgb_gray(label.color))\n",
        "  color_id_list.append((key_list, label.id))\n",
        "  color_id[key] = label.id\n",
        "  id_r[label.id] = label.color[0]\n",
        "  id_g[label.id] = label.color[1]\n",
        "  id_b[label.id] = label.color[2]\n",
        "color_id[0] = 0\n",
        "color_id[16] = 26 # grayscale value 16 has category \"car\"\n",
        "\n",
        "\n",
        "#tf.image.rgb_to_grayscale\n",
        "#convert dictionary to tf StaticHashTable\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(color_id.keys())), tf.convert_to_tensor((list(color_id.values()))))\n",
        "gray2label = tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_r.keys())), tf.convert_to_tensor((list(id_r.values()))))\n",
        "label2r =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_g.keys())), tf.convert_to_tensor((list(id_g.values()))))\n",
        "label2g =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_b.keys())), tf.convert_to_tensor((list(id_b.values()))))\n",
        "label2b =  tf.lookup.StaticHashTable(init,default_value= -100)"
      ],
      "metadata": {
        "id": "52rKSHZGUQuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_rgb(labeled_im):\n",
        "  r = label2r.lookup(labeled_im)[:,:,:,0]\n",
        "  g = label2g.lookup(labeled_im)[:,:,:,0]\n",
        "  b = label2b.lookup(labeled_im)[:,:,:,0]\n",
        "\n",
        "  image = tf.stack([r,g,b], axis = 3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = normalize(image)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "n8b2pHOHdSEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_to_label(gen_output):\n",
        "  return tf.expand_dims(tf.math.argmax(gen_output, axis = -1, output_type = tf.dtypes.int32), -1)\n",
        "\n",
        "def label_to_sparse(labels):\n",
        "  return tf.squeeze(tf.one_hot(labels, depth = 35),3)"
      ],
      "metadata": {
        "id": "jVC1LJ1wGFow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  # Sparse Categorical Crossentropy\n",
        "  CE_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(target, gen_output)\n",
        "  total_gen_loss = gan_loss + CE_loss\n",
        "\n",
        "  return total_gen_loss, gan_loss, CE_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "metadata": {
        "id": "-3E5dkQxrgNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_semantics = Generators.GeneratorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "discriminator_semantics = Discriminators.DiscriminatorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "lr = 1e-5\n",
        "generator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "tx9xIeoZdWyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"semantics\", \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer_semantics,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_semantics,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)"
      ],
      "metadata": {
        "id": "dW-piaz9dass"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/finetune/ft_semantics-2\")"
      ],
      "metadata": {
        "id": "1nUZKcMToyV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetune SemanticsGAN"
      ],
      "metadata": {
        "id": "rt5kclUErjxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, input, tar, mask):  \n",
        "  prediction = model(input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  prediction = tf.math.argmax(prediction, axis = -1, output_type = tf.dtypes.int32)\n",
        "  prediction = label_to_rgb(tf.expand_dims(prediction, -1))\n",
        "  \n",
        "  input = tf.cast(input, tf.int32)\n",
        "  input = label_to_rgb(input)\n",
        "\n",
        "\n",
        "  display_list = [input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "inmBx8DVrmKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epoch(input_image, mask, target):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator_semantics(input_image, training=True)\n",
        "    gen_output_argmax = gen_to_label(gen_output) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "    disc_real_output = discriminator_semantics([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator_semantics([input_image, gen_output_argmax], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  # from inpainted patches, count pixels that are not equal to the ground truth\n",
        "  mask_size = tf.math.count_nonzero(mask)\n",
        "  mask = tf.cast(mask, tf.int32)\n",
        "  gen_acc = tf.math.count_nonzero(tf.math.not_equal(target*mask, gen_output_argmax*mask))/mask_size\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator_semantics.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminator_semantics.trainable_variables)\n",
        "\n",
        "  generator_optimizer_semantics.apply_gradients(zip(generator_gradients, generator_semantics.trainable_variables))\n",
        "  discriminator_optimizer_semantics.apply_gradients(zip(discriminator_gradients, discriminator_semantics.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc\n",
        "    \n",
        "  \n",
        "def getValAccLoss(input_image, mask, target): \n",
        "  gen_output = generator_semantics(input_image, training=True)\n",
        "  gen_output_argmax = gen_to_label(gen_output) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminator_semantics([input_image, target], training=True)\n",
        "  disc_generated_output = discriminator_semantics([input_image, gen_output_argmax], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  mask_size = tf.math.count_nonzero(mask)\n",
        "  mask = tf.cast(mask, tf.int32)\n",
        "  gen_acc = tf.math.count_nonzero(tf.math.not_equal(target*mask, gen_output_argmax*mask))/mask_size\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc"
      ],
      "metadata": {
        "id": "RWnB7jnVrnyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/SemanticInpainting/epoch/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Train\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Val\")"
      ],
      "metadata": {
        "id": "MEE2mR0_roDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_ds, test_ds, epochs):\n",
        "  #image, masked_image, labels, masked_labels, mask\n",
        "  image_target, image_masked_test, label_test, masked_label_test, mask_test = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_CE_loss\":0, \"disc_loss\":0, \"gen_acc\":0}\n",
        "    generate_images(generator_semantics, masked_label_test, image_target, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(image, masked_image, label, masked_label, mask) in train_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc = train_epoch(masked_label, mask, label)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_CE_loss', metrics_dict[\"gen_CE_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_acc', metrics_dict[\"gen_acc\"]/tf.cast(total_ims, tf.float64), step=epoch)\n",
        "\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_CE_loss\":0, \"disc_loss\":0, \"gen_acc\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(image, masked_image, label, masked_label, mask) in test_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc = getValAccLoss(masked_label, mask, label)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_CE_loss', metrics_dict_val[\"gen_CE_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_acc', metrics_dict_val[\"gen_acc\"]/tf.cast(total_ims_val, tf.float64), step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "    #if epoch % 10 == 0 & epoch !=0: \n",
        "    #  checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "MWBqzGAdrol4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "metadata": {
        "id": "ZksFj0GLyZD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(finetune_train, finetune_val, 5)"
      ],
      "metadata": {
        "id": "FsLRs15Frsvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SemGAN"
      ],
      "metadata": {
        "id": "MAuYZQaQd1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss( disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # mean absolute error\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "W1smr4t4eArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generatorSEMGAN = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)"
      ],
      "metadata": {
        "id": "8neikwRLeDxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-5\n",
        "generator_optimizer = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.7)\n",
        "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "iJ10AD4EeD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint_dir_SEMGAN = '/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/'\n",
        "#checkpoint_prefix_SEMGAN = os.path.join(checkpoint_dir_SEMGAN, \"ckpt\", \"sparse\")\n",
        "checkpoint_SEMGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN,\n",
        "                                 discriminator=discriminatorSEMGAN)"
      ],
      "metadata": {
        "id": "ipBICX2teJLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_SEMGAN.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/SEMGAN/SemGAN-1\")"
      ],
      "metadata": {
        "id": "80udnxFNgfe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_test, masked_image_test, semantics_test, semantics_masked_test, mask_test = next(iter(finetune_val.take(1)))\n",
        "semantics_sparse_test = label_to_sparse(semantics_test)"
      ],
      "metadata": {
        "id": "RjtMeAu_xNJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(denormalize(masked_image_test[0]))"
      ],
      "metadata": {
        "id": "ngqnlBs2yDTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = generatorSEMGAN([masked_image_test, semantics_sparse_test], training=True)\n",
        "prediction = prediction*mask_test+image_test*(1-mask_test)\n",
        "plt.imshow(denormalize(prediction[0]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y-dFjrxexW45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training SemGAN"
      ],
      "metadata": {
        "id": "obLbfO4beNcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, masked_input, tar, semantics_prediction, semantics_sparse, mask):\n",
        "  prediction = model([masked_input, semantics_sparse], training=True)\n",
        "  prediction = prediction*mask+tar*(1-mask)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  semantics_prediction = label_to_rgb(semantics_prediction)\n",
        "\n",
        "  display_list = [masked_input[0], tar[0], prediction[0], semantics_prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image', 'Input Semantics']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "u2ol-bSXePee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epochSEMGAN(input_image, target, semantics, mask):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "    disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generatorSEMGAN.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminatorSEMGAN.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generatorSEMGAN.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminatorSEMGAN.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLossSEMGAN(input_image, target, semantics, mask): \n",
        "  gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "  disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss"
      ],
      "metadata": {
        "id": "BveAWFclePhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/finetune/SemGAN_GT/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Train\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Val\")"
      ],
      "metadata": {
        "id": "iSyn2LJZePjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitSEMGAN(train_ds, test_ds, epochs):\n",
        "  image_test, masked_image_test, semantics_test, semantics_masked_test, mask_test = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    \n",
        "    #semantics used for the example\n",
        "    semantics_predicted_test = gen_to_label(generator_semantics(semantics_masked_test, training=True)) # turn the generator output into labels\n",
        "    semantics_predicted_test = semantics_predicted_test*tf.cast(mask_test, tf.int32)+semantics_test*(1-tf.cast(mask_test, tf.int32)) # combine the prediction with the ground truth\n",
        "    semantics_sparse_test = label_to_sparse(semantics_predicted_test) # make a sparse matrix as input to the generator\n",
        "    #semantics_sparse_test = label_to_sparse(semantics_test)\n",
        "\n",
        "    generate_images(generatorSEMGAN, masked_image_test, image_test, semantics_predicted_test, semantics_sparse_test, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(image, masked_image, semantics, semantics_masked, mask) in train_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True)) # turn the generator output into labels\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32)) # combine the prediction with the ground truth\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted) # make a sparse matrix as input to the generator\n",
        "      #semantics = label_to_sparse(semantics)\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epochSEMGAN(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict[\"gen_L1_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(image, masked_image, semantics, semantics_masked, mask) in test_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True))\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32))\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted)\n",
        "      #semantics = label_to_sparse(semantics)\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = getValAccLossSEMGAN(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict_val[\"gen_L1_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "    #if epoch % 10 == 0 & epoch !=0: \n",
        "    #  checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "pKltqV__eTDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit"
      ],
      "metadata": {
        "id": "CPI2__3Efric"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "metadata": {
        "id": "tTL8VbZFfat8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitSEMGAN(finetune_train, finetune_val, 7)"
      ],
      "metadata": {
        "id": "W6t6T_NFftLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_SEMGAN.save(\"/content/drive/MyDrive/Scriptie/ckpt/finetune/SEMGAN/SEMGAN-2\")"
      ],
      "metadata": {
        "id": "XbpgprGLq6z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparison"
      ],
      "metadata": {
        "id": "3DyrNgAB-pHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generators.Generator(OUTPUT_CHANNELS)\n",
        "discriminator = Discriminators.Discriminator()\n",
        "\n",
        "generator_semantics = Generators.GeneratorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "discriminator_semantics = Discriminators.DiscriminatorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "generatorSEMGAN = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "generatorSEMGAN_GT = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN_GT = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)"
      ],
      "metadata": {
        "id": "PJ1lDACU-uNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-5\n",
        "generator_optimizer = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "dfnsehYvdD28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "checkpointSem = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)\n",
        "\n",
        "checkpointSEMGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN,\n",
        "                                 discriminator=discriminatorSEMGAN)\n",
        "\n",
        "checkpointSEMGAN_GT = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN_GT,\n",
        "                                 discriminator=discriminatorSEMGAN_GT)\n",
        "\n",
        "checkpointGAN.restore(\"/content/drive/MyDrive/Scriptie/ckpt/finetune/GAN/ckpt-2_2e-5\")\n",
        "checkpointSem.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/finetune/ft_semantics-2\")\n",
        "checkpointSEMGAN.restore(\"/content/drive/MyDrive/Scriptie/ckpt/finetune/SEMGAN/SEMGAN-2-2\")\n",
        "checkpointSEMGAN_GT.restore(\"/content/drive/MyDrive/Scriptie/ckpt/finetune/SEMGAN_GT/SEMGAN_GT-2\")"
      ],
      "metadata": {
        "id": "wodtnaq1JOQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_test, masked_image_test, semantics_test, semantics_masked_test, mask_test = next(iter(finetune_val.take(1)))\n",
        "sparse_semantics = label_to_sparse(semantics_test)\n",
        "pred = generatorSEMGAN_GT([masked_image_test, sparse_semantics], True)\n",
        "pred = pred * mask_test + image_test * (1-mask_test)\n",
        "plt.imshow(pred[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "PUV3CLclbg8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "mxBdC9TDgF5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the metrics of the dataset. The output of *load_images_cs()* is: *image, masked_image, labels, masked_labels, mask*"
      ],
      "metadata": {
        "id": "ogARt2WogHOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(image_1, image_2):\n",
        "  # calculate L1 distance\n",
        "  L1_distance = tf.math.reduce_sum(tf.math.abs(image_1 - image_2))\n",
        "  ssim = tf.image.ssim(image_1, image_2, 2.0)\n",
        "  psnr = tf.image.psnr(image_1, image_2, 2.0)\n",
        "  return [L1_distance.numpy(), ssim.numpy()[0], psnr.numpy()[0]]"
      ],
      "metadata": {
        "id": "7hWQ2M-IgFYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getIoU(label, predicted_label, mask):\n",
        "  _, shape_h, shape_w, _ = label.shape\n",
        "  unique_labels, _ = tf.unique(tf.reshape(label, shape_h*shape_w))\n",
        "  label = tf.boolean_mask(label, mask)\n",
        "  predicted_label = tf.boolean_mask(predicted_label, mask)\n",
        "  IoU = tf.keras.metrics.IoU(num_classes=35, target_class_ids = unique_labels)(label, predicted_label)\n",
        "  return IoU.numpy()"
      ],
      "metadata": {
        "id": "dwZusDwkrSYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_metrics(ds):\n",
        "  #input_image is the inpainted image, GT is the groundtruth\n",
        "  names = [\"L1\",\"SSIM\",\"PSNR\"]\n",
        "  metrics_list_GAN = []\n",
        "  metrics_list_SEMGAN = []\n",
        "  metrics_list_SEMGAN_GT = []\n",
        "  mask_list = []\n",
        "  GT_list = []\n",
        "  IoU_list = []\n",
        "  path = \"/content/predictions\"\n",
        "\n",
        "  for step, (image, masked_image, semantics, semantics_masked, mask) in enumerate(ds):\n",
        "    display.clear_output(wait=True)\n",
        "    prediction = generator(masked_image, training = True)\n",
        "    prediction = prediction * mask + image*(1-mask)\n",
        "    target_path_im = os.path.join(path, \"GAN\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "    \n",
        "    metrics_list_GAN.append(metrics(image, prediction))\n",
        "    GT_list.append(image)\n",
        "    mask_list.append(mask)\n",
        "\n",
        "    semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training = True))\n",
        "    mask_int =  tf.cast(mask, tf.int32)\n",
        "    semantics_predicted = semantics_predicted * mask_int + semantics * (1-mask_int)\n",
        "    semantics_predicted_sparse = label_to_sparse(semantics_predicted)\n",
        "    IoU = getIoU(semantics, semantics_predicted, mask)\n",
        "    \n",
        "    prediction = generatorSEMGAN([masked_image, semantics_predicted_sparse], training=True)\n",
        "    prediction = prediction * mask + image*(1-mask)\n",
        "    \n",
        "    target_path_im = os.path.join(path, \"SemGAN\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "    target_path_sem = os.path.join(path, \"SemGAN\", \"semantics\", str(step) + \".png\")\n",
        "    semantics_predicted = label_to_rgb(semantics_predicted)\n",
        "    tf.keras.utils.save_img(target_path_sem, semantics_predicted[0])\n",
        "\n",
        "    metrics_list_SEMGAN.append(metrics(image, prediction))\n",
        "    IoU_list.append(IoU)\n",
        "\n",
        "    semantics_sparse = label_to_sparse(semantics)\n",
        "    prediction = generatorSEMGAN_GT([masked_image, semantics_sparse], training=True) #estimated values for each of 35 classes\n",
        "    prediction = prediction * mask + image*(1-mask) #insert generated label patches to inverted ground truth\n",
        "    target_path_im = os.path.join(path, \"SemGAN_GT\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "\n",
        "    target_path_sem = os.path.join(path, \"SemGAN_GT\", \"semantics\", str(step) + \".png\")\n",
        "    semantics = label_to_rgb(semantics)\n",
        "    tf.keras.utils.save_img(target_path_sem, semantics[0])\n",
        "                            \n",
        "    metrics_list_SEMGAN_GT.append(metrics(image, prediction))\n",
        "\n",
        "    print(step)\n",
        "    \n",
        "  \n",
        "  metrics_df_GAN = pd.DataFrame(metrics_list_GAN)\n",
        "  metrics_df_GAN.columns = names\n",
        "  metrics_df_GAN[\"GT\"] = GT_list\n",
        "  metrics_df_GAN[\"Mask\"] = mask_list\n",
        "\n",
        "  metrics_df_SEMGAN = pd.DataFrame(metrics_list_SEMGAN)\n",
        "  metrics_df_SEMGAN.columns = names\n",
        "  metrics_df_SEMGAN[\"IoU\"] = IoU_list\n",
        "\n",
        "  metrics_df_SEMGAN_GT = pd.DataFrame(metrics_list_SEMGAN_GT)\n",
        "  metrics_df_SEMGAN_GT.columns = names\n",
        "\n",
        "  return metrics_df_GAN, metrics_df_SEMGAN, metrics_df_SEMGAN_GT"
      ],
      "metadata": {
        "id": "U61XVdExgKbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/predictions\n",
        "! mkdir /content/predictions/GAN\n",
        "! mkdir /content/predictions/SemGAN\n",
        "! mkdir /content/predictions/SemGAN_GT\n",
        "! mkdir /content/predictions/SemGAN/semantics\n",
        "! mkdir /content/predictions/SemGAN_GT/semantics"
      ],
      "metadata": {
        "id": "bhHzqTVUnCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_GAN, metrics_ft_SEMGAN, metrics_ft_SEMGAN_GT = get_dataset_metrics(finetune_val)"
      ],
      "metadata": {
        "id": "Hi6LG8u_owHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN[\"IoU\"].mean()"
      ],
      "metadata": {
        "id": "4UTen3TRDNXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics_ft_GAN[\"L1\"].mean())\n",
        "print(metrics_ft_GAN[\"PSNR\"].mean())\n",
        "print(metrics_ft_GAN[\"SSIM\"].mean())\n",
        "print(metrics_ft_GAN[\"L1\"].std())\n",
        "print(metrics_ft_GAN[\"PSNR\"].std())\n",
        "print(metrics_ft_GAN[\"SSIM\"].std())"
      ],
      "metadata": {
        "id": "QJ-jmD10-Rtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics_ft_SEMGAN[\"L1\"].mean())\n",
        "print(metrics_ft_SEMGAN[\"PSNR\"].mean())\n",
        "print(metrics_ft_SEMGAN[\"SSIM\"].mean())\n",
        "print(metrics_ft_SEMGAN[\"L1\"].std())\n",
        "print(metrics_ft_SEMGAN[\"PSNR\"].std())\n",
        "print(metrics_ft_SEMGAN[\"SSIM\"].std())"
      ],
      "metadata": {
        "id": "8kvtK2R37qjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics_ft_SEMGAN_GT[\"L1\"].mean())\n",
        "print(metrics_ft_SEMGAN_GT[\"PSNR\"].mean())\n",
        "print(metrics_ft_SEMGAN_GT[\"SSIM\"].mean())\n",
        "print(metrics_ft_SEMGAN_GT[\"L1\"].std())\n",
        "print(metrics_ft_SEMGAN_GT[\"PSNR\"].std())\n",
        "print(metrics_ft_SEMGAN_GT[\"SSIM\"].std())"
      ],
      "metadata": {
        "id": "SkqJlUIjULFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN[[\"L1\", \"PSNR\", \"SSIM\"]]"
      ],
      "metadata": {
        "id": "ix36YnrmKsrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN"
      ],
      "metadata": {
        "id": "L6vRUciIsQm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN_GT"
      ],
      "metadata": {
        "id": "k33dUKNDsR1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the images"
      ],
      "metadata": {
        "id": "4zo8IZXvglYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = load(\"/content/predictions/GAN/1.png\", 3)\n",
        "plt.imshow(normalize(im)*0.5+0.5)"
      ],
      "metadata": {
        "id": "Ot9IAI6OyN_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_rows(row):\n",
        "  path = \"/content/predictions\"\n",
        "  GT = metrics_ft_GAN.iloc[row][\"GT\"]\n",
        "  mask = metrics_ft_GAN.iloc[row][\"Mask\"]\n",
        "  inp_im = tf.where(mask == 1, -1, GT)\n",
        "  \n",
        "\n",
        "  GAN_pred = normalize(load(os.path.join(path, \"GAN\", str(row)+\".png\"), 3))\n",
        "  SEMGAN_pred = normalize(load(os.path.join(path, \"SemGAN\", str(row)+\".png\"), 3))\n",
        "  SEMGAN_GT_pred = normalize(load(os.path.join(path, \"SemGAN_GT\", str(row)+\".png\"), 3))\n",
        "\n",
        "  semantics_pred = load(os.path.join(path, \"SemGAN\", \"semantics\", str(row)+\".png\"), 3)\n",
        "  semantics_pred = normalize(semantics_pred)\n",
        "  semantics = load(os.path.join(path, \"SemGAN_GT\", \"semantics\", str(row)+\".png\"), 3)\n",
        "\n",
        "  fix, axs = plt.subplots(6,1, figsize = (20,20))\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  \n",
        "  images = [inp_im[0], semantics_pred, GT[0], GAN_pred, SEMGAN_pred, SEMGAN_GT_pred]\n",
        "  for i, im in enumerate(images):\n",
        "    axs[i].imshow(im*0.5+0.5)\n",
        "    axs[i].axis(\"off\")\n",
        "\n"
      ],
      "metadata": {
        "id": "M3M1Nx_btBow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_rows(0)"
      ],
      "metadata": {
        "id": "CU6w0Yv24Vkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pd_visualize_rows(ds, rows):\n",
        "  n = len(rows)\n",
        "  fix, axs = plt.subplots(n,3, figsize = (20,20))\n",
        "  plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "  for i, row in enumerate(rows):\n",
        "    l1, ssim, psnr, image, semantics, mask = ds.iloc[row]\n",
        "    masked_image = tf.where(mask == 1, -1, image)\n",
        "    prediction = generator(masked_image, training = True)\n",
        "    prediction = prediction * mask + image * (1-mask)\n",
        "\n",
        "    if n > 1:\n",
        "      axs[i][0].imshow(denormalize(image[0]))\n",
        "      axs[i][1].imshow(denormalize(masked_image[0]))\n",
        "      axs[i][2].imshow(denormalize(prediction[0]))\n",
        "      for ax in axs[i]:\n",
        "          ax.axis(\"off\")\n",
        "    else:\n",
        "      axs[0].imshow(denormalize(image[0]))\n",
        "      axs[1].imshow(denormalize(masked_image[0]))\n",
        "      axs[2].imshow(denormalize(prediction[0]))\n",
        "\n",
        "      for ax in axs:\n",
        "          ax.axis(\"off\")\n",
        "      \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "N1Etht3_gk0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_images():\n",
        "  path = \"/content/predictions\"\n",
        "  "
      ],
      "metadata": {
        "id": "iIsugQ1deWja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}