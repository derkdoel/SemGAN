{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVfVVm54u4QE"
      },
      "source": [
        "#GAN with semantics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXzhAPd9utco"
      },
      "outputs": [],
      "source": [
        "! mkdir /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsbi4wLIuwrp"
      },
      "outputs": [],
      "source": [
        "! wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=d.a.vandendoel@students.uu.nl&password=Derkojo95!&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1 -P /content/cityScapes\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3 -P /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQd_RlA1uzjd"
      },
      "outputs": [],
      "source": [
        "! unzip /content/cityScapes/gtFine_trainvaltest.zip -d /content/cityScapes/annotations\n",
        "! unzip /content/cityScapes/leftImg8bit_trainvaltest.zip -d /content/cityScapes/img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z_3hyt8wOUE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOnNeN5Lu17M"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCgjk5X3u2I7"
      },
      "outputs": [],
      "source": [
        "#from __future__ import print_function, division\n",
        "import cv2\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "import PIL\n",
        "import scipy\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "from tqdm import tqdm\n",
        "from random import randint, seed\n",
        "from google.colab.patches import cv2_imshow\n",
        "from imageio import imread\n",
        "from tensorflow.keras import layers\n",
        "from glob import glob\n",
        "from IPython import display\n",
        "from collections import namedtuple\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LogNorm, Normalize\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgBpEXVk5uq"
      },
      "source": [
        "#New mask function (Do not use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMjFxwlD_ccU"
      },
      "outputs": [],
      "source": [
        "def mask_patch(image, patchsize, location_h, location_w):\n",
        "  image_h, image_w, image_d = image.shape #batch, height, width, depth\n",
        "  patch_h, patch_w = patchsize\n",
        "  patch = tf.ones((patch_h, patch_w, 3))\n",
        "\n",
        "  pad_left = tf.zeros((patch_h, location_w, 3)) #pad the image with zeros from the left border of the image until the patch\n",
        "  pad_right = tf.zeros((patch_h, image_w-(location_w+patch_w), 3)) #pad the image with zeros from the patch until the right border of the image\n",
        "  \n",
        "  print(f\"pad left shape: {pad_left.shape}, patch shape: {patch.shape}, pad right shape: {pad_right.shape}\")\n",
        "\n",
        "  new_patch = tf.concat([pad_left, patch, pad_right], axis =1)\n",
        "  new_patch_w = new_patch.shape[1]\n",
        "\n",
        "  pad_up = tf.zeros((location_h, new_patch_w, 3))\n",
        "  pad_down = tf.zeros((image_h - (location_h+patch_h), new_patch_w, 3))\n",
        "  \n",
        "  new_patch = tf.concat([pad_up, new_patch, pad_down], axis=0)\n",
        "\n",
        "  return new_patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPPtjIVtGqiI"
      },
      "outputs": [],
      "source": [
        "def image_mask(image, mask_height, mask_width, num_masks, rand_seed = None):\n",
        "  # random_cutout function needs mask_height * mask_width to be divisible by 2\n",
        "  if (mask_height * mask_width)%2 != 0:\n",
        "    raise Exception(\"Error! mask size must be divisible by 2\")\n",
        "  \n",
        "\n",
        "  im_height = tf.cast(IM_HEIGHT, dtype=tf.dtypes.int32)\n",
        "  im_width = tf.cast(IM_WIDTH, dtype=tf.dtypes.int32)  \n",
        "  mask_height = tf.cast(mask_height, dtype=tf.dtypes.int32)\n",
        "  mask_width = tf.cast(mask_width, dtype=tf.dtypes.int32)\n",
        "  \n",
        "  min_height = tf.cast(tf.math.round(im_height/5), tf.dtypes.int32)\n",
        "  min_width = tf.cast(tf.math.round(mask_width/2), tf.dtypes.int32)\n",
        "\n",
        "  try:\n",
        "    image_shape = image.shape\n",
        "  except:\n",
        "    image_shape = image.get_shape()\n",
        "\n",
        "  if len(image_shape) == 3:\n",
        "    image = tf.expand_dims(image, 0)\n",
        "  elif len(image_shape) == 4:\n",
        "    pass\n",
        "  else:\n",
        "    raise Exception('Error! Tensor shape must either be: (batch_size, image height, image width, color channels) or (image height, image width, color channels)')\n",
        "  \n",
        "  mask = tf.ones_like(image, tf.dtypes.float32)[0,:,:,0:1]\n",
        "\n",
        "  for i in range(num_masks):\n",
        "    height_offset = tf.random.uniform(shape = (), minval= min_height, maxval=im_height-mask_height, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    width_offset = tf.random.uniform(shape = (), minval= 0, maxval= im_width-mask_width, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    print(f\"height_offset is: {height_offset}, width_offset: {width_offset}, mask_height: {mask_height}, mask_width: {mask_width}\")\n",
        "    mask = mask_patch(mask, (mask_height, mask_width),  height_offset, width_offset)\n",
        "  \n",
        "  masked_image = (1-mask) * image\n",
        "\n",
        "  return masked_image[0], mask[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XmDPCTFV8jk"
      },
      "source": [
        "#Load JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCtxGMMNeGNg"
      },
      "outputs": [],
      "source": [
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (246, 35,232) ), #244,35,232\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ), #107 142 35\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]\n",
        "\n",
        "#convert rgb to grayscale values, this produces a unique grayscale value for each label\n",
        "def rgb_gray(rgb):\n",
        "  r, g ,b = rgb\n",
        "  return 0.2989*r + 0.5870*g + 0.1140*b\n",
        "\n",
        "#create dictionary with grayscale as key and id as value\n",
        "color_id_list = []\n",
        "color_id = {}\n",
        "id_r = {}\n",
        "id_g = {}\n",
        "id_b = {}\n",
        "for label in labels:\n",
        "  key_list = tf.image.rgb_to_grayscale(tf.convert_to_tensor(label.color))\n",
        "  key = int(rgb_gray(label.color))\n",
        "  color_id_list.append((key_list, label.id))\n",
        "  color_id[key] = label.id\n",
        "  id_r[label.id] = label.color[0]\n",
        "  id_g[label.id] = label.color[1]\n",
        "  id_b[label.id] = label.color[2]\n",
        "color_id[0] = 0\n",
        "color_id[16] = 26 # grayscale value 16 has category \"car\"\n",
        "\n",
        "\n",
        "#tf.image.rgb_to_grayscale\n",
        "#convert dictionary to tf StaticHashTable\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(color_id.keys())), tf.convert_to_tensor((list(color_id.values()))))\n",
        "gray2label = tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_r.keys())), tf.convert_to_tensor((list(id_r.values()))))\n",
        "label2r =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_g.keys())), tf.convert_to_tensor((list(id_g.values()))))\n",
        "label2g =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_b.keys())), tf.convert_to_tensor((list(id_b.values()))))\n",
        "label2b =  tf.lookup.StaticHashTable(init,default_value= -100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLrCbfBgwcDR"
      },
      "source": [
        "# Image pre-processing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "950iYRLau96f"
      },
      "outputs": [],
      "source": [
        "IM_HEIGHT = 256\n",
        "IM_WIDTH = 512\n",
        "BUFFER_SIZE = 2975\n",
        "BATCH_SIZE = 1\n",
        "OUTPUT_CHANNELS = 3\n",
        "LAMBDA = 100\n",
        "MASK_HEIGTH = 64\n",
        "MASK_WIDTH = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbLh1XUSu_QD"
      },
      "outputs": [],
      "source": [
        "def load(image_file):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.io.decode_png(image, channels=3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLs6_h5hvAIo"
      },
      "outputs": [],
      "source": [
        "def resize(input_image, height, width):\n",
        "    image = tf.image.resize(\n",
        "        input_image, \n",
        "        [height, width],\n",
        "        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
        "    )\n",
        "  \n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPSz3v8SvALc"
      },
      "outputs": [],
      "source": [
        "def normalize(image):\n",
        "  image = image / 127.5 - 1\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDNMW1PRvDMP"
      },
      "outputs": [],
      "source": [
        "def image_mask(image, mask_height, mask_width, num_masks, rand_seed = None):\n",
        "  # random_cutout function needs mask_height * mask_width to be divisible by 2\n",
        "  if (mask_height * mask_width)%2 != 0:\n",
        "    raise Exception(\"Error! mask size must be divisible by 2\")\n",
        "  \n",
        "  im_height = tf.cast(IM_HEIGHT, dtype=tf.dtypes.int32)\n",
        "  im_width = tf.cast(IM_WIDTH, dtype=tf.dtypes.int32)  \n",
        "  mask_height = tf.cast(mask_height, dtype=tf.dtypes.int32)\n",
        "  mask_width = tf.cast(mask_width, dtype=tf.dtypes.int32)\n",
        "  \n",
        "  min_height = tf.cast(tf.math.round((mask_height/2)+15), tf.dtypes.int32)\n",
        "  min_width = tf.cast(tf.math.round((mask_width/2)+10), tf.dtypes.int32)\n",
        "  max_height = tf.cast(tf.math.round(im_height/10), tf.dtypes.int32)\n",
        "\n",
        "  try:\n",
        "    image_shape = image.shape\n",
        "  except:\n",
        "    image_shape = image.get_shape()\n",
        "\n",
        "  if len(image_shape) == 3:\n",
        "    image = tf.expand_dims(image, 0)\n",
        "  elif len(image_shape) == 4:\n",
        "    pass\n",
        "  else:\n",
        "    raise Exception('Error! Tensor shape must either be: (batch_size, image height, image width, color channels) or (image height, image width, color channels)')\n",
        "  \n",
        "  mask = tf.zeros_like(image, tf.dtypes.float32)[:,:,:,0:1]\n",
        "  locations = []\n",
        "\n",
        "  #create masks\n",
        "  for i in range(num_masks):\n",
        "    mask_height_random = tf.random.uniform(shape = (), minval= mask_height-5, maxval=mask_height+15, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask_height_random = mask_height_random if mask_height_random%2 == 0 else mask_height_random-1\n",
        "    mask_width_random = tf.random.uniform(shape = (), minval= mask_width, maxval=mask_width+10, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask_width_random = mask_width_random if mask_width_random%2 == 0 else mask_width_random-1\n",
        "    \n",
        "    height_offset = tf.random.uniform(shape = (), minval= min_height, maxval=im_height-max_height, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    width_offset = tf.random.uniform(shape = (), minval= min_width, maxval= im_width-min_width, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask = tfa.image.cutout(mask, (mask_height_random, mask_width_random), (height_offset, width_offset), constant_values = 1)\n",
        "    locations.append((mask_height_random, mask_width_random, height_offset, width_offset))\n",
        "  \n",
        "  masked_image = (1-mask) * image\n",
        "\n",
        "    #mask = tf.zeros([image_heigt, image_width])\n",
        "    #mask = mask[height_offset:height_offset+mask_height].assign(tf.ones([mask_height, mask_width]))\n",
        "    #image = mask * image\n",
        "\n",
        "\n",
        "  return masked_image[0], mask[0], locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgE3P6wtlSa9"
      },
      "outputs": [],
      "source": [
        "def label_to_rgb(labeled_im):\n",
        "  r = label2r.lookup(labeled_im)[:,:,:,0]\n",
        "  g = label2g.lookup(labeled_im)[:,:,:,0]\n",
        "  b = label2b.lookup(labeled_im)[:,:,:,0]\n",
        "\n",
        "  image = tf.stack([r,g,b], axis = 3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = normalize(image)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJHgGLG9xUTQ"
      },
      "outputs": [],
      "source": [
        "def load_images(semantics_file, label_file):\n",
        "  semantics = load(semantics_file)\n",
        "  semantics = resize(semantics, 256, 512)\n",
        "  semantics = normalize(semantics)\n",
        "  masked_semantics, mask, locations = image_mask(semantics, MASK_HEIGTH, MASK_WIDTH, 4)\n",
        "  \n",
        "  mask = tf.cast(mask, tf.int32)\n",
        "  label = tf.io.read_file(label_file)\n",
        "  label = tf.io.decode_png(label, channels = 0)\n",
        "  label = tf.cast(label, tf.int32)\n",
        "  label = resize(label, 256, 512)\n",
        "  masked_label = (1-mask)*label\n",
        "\n",
        "  return masked_label, label, semantics, masked_semantics, mask, label_file, locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sst0fJYuk2CU"
      },
      "source": [
        "#Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rFuaSm1tllO"
      },
      "outputs": [],
      "source": [
        "# Fill pandas dataframe with the paths to the files\n",
        "def folder_to_pd(semantics_dir):\n",
        "  semantics_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "  label_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "\n",
        "  for data_type in [\"test\", \"train\", \"val\"]:\n",
        "    semantics_subdir = os.path.join(semantics_dir, data_type)\n",
        "    for root, subdir, files in os.walk(semantics_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for i,file in enumerate(files):\n",
        "          if file[-9:] == \"color.png\":\n",
        "            semantics_paths[data_type].append(os.path.join(root, file))\n",
        "          if file[-12:] == \"labelIds.png\":\n",
        "            label_paths[data_type].append(os.path.join(root, file))\n",
        "  \n",
        "  \n",
        "  test_ds = pd.DataFrame(list(zip(semantics_paths[\"test\"], label_paths[\"test\"])), columns =['Semantics', 'label'])\n",
        "  train_ds = pd.DataFrame(list(zip(semantics_paths[\"train\"], label_paths[\"train\"])), columns =['Semantics', 'label'])\n",
        "  val_ds = pd.DataFrame(list(zip(semantics_paths[\"val\"], label_paths[\"val\"])), columns =['Semantics', 'label'])\n",
        "\n",
        "  return test_ds, train_ds, val_ds\n",
        "\n",
        "test_ds, train_ds, val_ds = folder_to_pd(\"/content/cityScapes/annotations/gtFine\")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_ds[\"Semantics\"], test_ds[\"label\"]))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_ds[\"Semantics\"], train_ds[\"label\"]))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_ds[\"Semantics\"], val_ds[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcEpAI9yuitS"
      },
      "outputs": [],
      "source": [
        "tf_train = train_ds.shuffle(train_ds.cardinality(), reshuffle_each_iteration=False)\n",
        "#tf_train = tf_train.interleave(tf.data.TFRecordDataset, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "tf_train = tf_train.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_train = tf_train.batch(BATCH_SIZE)\n",
        "tf_train = tf_train.cache(\"/content/temporary.tfcache\")\n",
        "tf_train = tf_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEUCy535qe2I"
      },
      "outputs": [],
      "source": [
        "tf_val = val_ds.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_val = tf_val.batch(BATCH_SIZE)\n",
        "tf_val = tf_val.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7bXYuAVJ-h"
      },
      "outputs": [],
      "source": [
        "for masked_label, label, image, masked_image, mask, image_file, locations in tf_val.take(1):\n",
        "  plt.imshow(label_to_rgb(label)[0]*0.5+0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I8Tmh6I_JRk"
      },
      "outputs": [],
      "source": [
        "plt.imshow(label_to_rgb(masked_label)[0]*0.5+0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OP2Tp0Ix_T"
      },
      "source": [
        "#Label GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2J9AaZLIx_T"
      },
      "outputs": [],
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                                kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "        result.add(tf.keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)))\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOrqUWtIx_T"
      },
      "outputs": [],
      "source": [
        "def upsample(filters, size, h, w, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "    result.add(tf.keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)))\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GeneratorSemantics():\n",
        "        down_stack = [\n",
        "            downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 256, 64)\n",
        "            downsample(128, 4),  # (bs, 64, 128, 128)\n",
        "            downsample(256, 4),  # (bs, 32, 64, 256)\n",
        "            downsample(512, 4),  # (bs, 16, 32, 512)\n",
        "            downsample(512, 4),  # (bs, 8, 16, 512)\n",
        "            downsample(512, 4),  # (bs, 4, 8, 512)\n",
        "            downsample(512, 4),  # (bs, 2, 4, 512)\n",
        "            downsample(512, 4),  # (bs, 1, 2, 512)\n",
        "        ]\n",
        "\n",
        "        up_stack = [\n",
        "            upsample(512, 4, 1, 2, apply_dropout=True),  # (bs, 2, 4, 1024)\n",
        "            upsample(512, 4, 2, 4, apply_dropout=True),  # (bs, 4, 8, 1024)\n",
        "            upsample(512, 4, 4, 8, apply_dropout=True),  # (bs, 8, 16, 1024)\n",
        "            upsample(512, 4, 8, 16),  # (bs, 16, 32, 1024)\n",
        "            upsample(256, 4, 16, 32),  # (bs, 32, 64, 512)\n",
        "            upsample(128, 4, 32, 64),  # (bs, 64, 128, 256)\n",
        "            upsample(64, 4, 64, 128),  # (bs, 128, 256, 128)\n",
        "        ]\n",
        "\n",
        "        initializer = tf.random_normal_initializer(0., 0.02)\n",
        "        last_upsample = upsample(64, 4, 128, 256)\n",
        "        last = tf.keras.layers.Conv2D(35, 4, strides=1, padding='same', kernel_initializer=initializer, activation='tanh')  # (bs, 256, 512, 35) 35 is the number of classes \n",
        "\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        semantics = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 1])\n",
        "\n",
        "        x = semantics\n",
        "\n",
        "        # Downsampling through the model\n",
        "        skips = []\n",
        "        for down in down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips = reversed(skips[:-1])\n",
        "\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip in zip(up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = concat([x, skip])\n",
        "\n",
        "        x = last_upsample(x)\n",
        "        x = last(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=semantics, outputs=x)"
      ],
      "metadata": {
        "id": "1vVqkEU5YPhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiscriminatorSemantics():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    semantics = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 1], name='semantics')\n",
        "    tar = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 1], name='target_image')\n",
        "\n",
        "    x = tf.keras.layers.concatenate([semantics, tar])  # (bs, 256, 512, channels*2)\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)  # (bs, 128, 256, 64)\n",
        "    down2 = downsample(128, 4)(down1)  # (bs, 64, 128, 128)\n",
        "    down3 = downsample(256, 4)(down2)  # (bs, 32, 64, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 66, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  use_bias=False)(zero_pad1)  # (bs, 31, 63, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 65, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                  kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 62, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[semantics, tar], outputs=last)"
      ],
      "metadata": {
        "id": "cCzSpy59YPkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQiC-fUBIx_U"
      },
      "outputs": [],
      "source": [
        "generator_semantics = GeneratorSemantics()\n",
        "discriminator_semantics = DiscriminatorSemantics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuTvKN9zvf4A"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  # Sparse Categorical Crossentropy\n",
        "  CE_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(target, gen_output)\n",
        "  total_gen_loss = gan_loss + CE_loss\n",
        "\n",
        "  return total_gen_loss, gan_loss, CE_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-ZZcJ67Ix_V"
      },
      "outputs": [],
      "source": [
        "lr = 2e-4\n",
        "generator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.75)\n",
        "discriminator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju5LaBIcIx_V"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"semantics\", \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer_semantics,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_semantics,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/semantics/ckpt-1\")"
      ],
      "metadata": {
        "id": "s3ZzlpEIhSEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD3DmkxbfQ38"
      },
      "outputs": [],
      "source": [
        "def gen_to_label(gen_output):\n",
        "  return tf.expand_dims(tf.math.argmax(gen_output, axis = -1, output_type = tf.dtypes.int32), -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_sparse(labels):\n",
        "  return tf.squeeze(tf.one_hot(tf_max, depth = 35),3)"
      ],
      "metadata": {
        "id": "TdtiJlb9fyHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HpF51xNho5s"
      },
      "source": [
        "# Generate images and train_step functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEpNj3ZG8qIl"
      },
      "outputs": [],
      "source": [
        "def generate_images(model, input, tar, mask):  \n",
        "  prediction = model(input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  prediction = tf.math.argmax(prediction, axis = -1, output_type = tf.dtypes.int32)\n",
        "  prediction = label_to_rgb(tf.expand_dims(prediction, -1))\n",
        "  \n",
        "  input = tf.cast(input, tf.int32)\n",
        "  input = label_to_rgb(input)\n",
        "\n",
        "\n",
        "  display_list = [input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k__WK3vcNtTE"
      },
      "source": [
        "#Fit (epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5IBcIQ8MhaX"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_epoch(input_image, mask, target):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator_semantics(input_image, training=True)\n",
        "    gen_output_argmax = gen_to_label(gen_output) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "    disc_real_output = discriminator_semantics([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator_semantics([input_image, gen_output_argmax], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  # from inpainted patches, count pixels that are not equal to the ground truth\n",
        "  mask_size = tf.math.count_nonzero(mask)\n",
        "  gen_acc = tf.math.count_nonzero(tf.math.not_equal(target*mask, tf.expand_dims(gen_output_argmax, -1)*mask))/mask_size\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator_semantics.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminator_semantics.trainable_variables)\n",
        "\n",
        "  generator_optimizer_semantics.apply_gradients(zip(generator_gradients, generator_semantics.trainable_variables))\n",
        "  discriminator_optimizer_semantics.apply_gradients(zip(discriminator_gradients, discriminator_semantics.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc\n",
        "    \n",
        "  \n",
        "def getValAccLoss(input_image, mask, target): \n",
        "  gen_output = generator_semantics(input_image, training=True)\n",
        "  gen_output_argmax = gen_to_label(gen_output) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminator_semantics([input_image, target], training=True)\n",
        "  disc_generated_output = discriminator_semantics([input_image, gen_output_argmax], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  mask_size = tf.math.count_nonzero(mask)\n",
        "  gen_acc = tf.math.count_nonzero(tf.math.not_equal(target*mask, tf.expand_dims(gen_output_argmax, -1)*mask))/mask_size\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/SemanticInpainting/epoch/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Train\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Val\")"
      ],
      "metadata": {
        "id": "ROWX2zF375Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtfCduxyNpDc"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds, test_ds, epochs):\n",
        "  masked_label_test, label_test, image_target, image_masked_test, mask_test, path = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_CE_loss\":0, \"disc_loss\":0, \"gen_acc\":0}\n",
        "    generate_images(generator_semantics, masked_label_test, image_target, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(masked_label, label, image, masked_image, mask, image_file) in train_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc = train_epoch(masked_label, mask, label)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_CE_loss', metrics_dict[\"gen_CE_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_acc', metrics_dict[\"gen_acc\"]/tf.cast(total_ims, tf.float64), step=epoch)\n",
        "\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_CE_loss\":0, \"disc_loss\":0, \"gen_acc\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(masked_label, label, image, masked_image, mask, image_file) in test_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc = getValAccLoss(masked_label, mask, label)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss, gen_acc)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_CE_loss', metrics_dict_val[\"gen_CE_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_acc', metrics_dict_val[\"gen_acc\"]/tf.cast(total_ims_val, tf.float64), step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "    #if epoch % 10 == 0 & epoch !=0: \n",
        "    #  checkpoint.save(file_prefix=checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIUi-Ngv1Mpe"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iASefj4qpaSv"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8g3J52cpgZr"
      },
      "outputs": [],
      "source": [
        "fit(tf_train, tf_val, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = gen_to_label(generator_semantics(masked_label))"
      ],
      "metadata": {
        "id": "gA1n_VE_xT2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(label_to_rgb(masked_label)[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "DNhGcz27zOD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(label_to_rgb(tf.expand_dims(prediction, -1))[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "Dek4xBT7zEHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(label_to_rgb(label)[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "j5lIbpbuMXQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion matrix"
      ],
      "metadata": {
        "id": "0WFTgevThfEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for masked_label, label, image, masked_image, mask, image_file, locations in tf_train.take(1):\n",
        "  prediction = gen_to_label(generator_semantics(masked_label, training = True)) * mask + label * (1-mask)\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(15, 15))\n",
        "  axs[0].imshow(label_to_rgb(label)[0]*0.5+0.5)\n",
        "  axs[1].imshow(label_to_rgb(prediction)[0]*0.5+0.5)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R_9Zxt3Biz88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(label_to_rgb(masked_label)[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "byjhbhh5xPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
        "IoU = tf.keras.metrics.IoU(num_classes=len(valid_labels), target_class_ids=valid_labels)"
      ],
      "metadata": {
        "id": "5F4icnuG7TkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un, _ = tf.unique(inp)"
      ],
      "metadata": {
        "id": "B6TT0v50ZZCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_mask = tf.boolean_mask(prediction, mask)\n",
        "label_mask = tf.boolean_mask(label, mask)"
      ],
      "metadata": {
        "id": "CtQu12if0OT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_flat = tf.reshape(label, [-1])\n",
        "prediction_flat = tf.reshape(prediction, [-1])"
      ],
      "metadata": {
        "id": "PIGWdfrlkuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfm = tf.math.confusion_matrix(label_mask, prediction_mask, num_classes=34)"
      ],
      "metadata": {
        "id": "5H-TAe9ly1LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (masked_label, label, image, masked_image, mask, image_file, locations) in tf_val.enumerate():\n",
        "  prediction = gen_to_label(generator_semantics(masked_label, training = True)) * mask + label * (1-mask)\n",
        "  prediction_mask = tf.boolean_mask(prediction, mask)\n",
        "  label_mask = tf.boolean_mask(label, mask)\n",
        "  \n",
        "  if step == 0:\n",
        "    pred = prediction_mask\n",
        "    inp = label_mask\n",
        "  else:\n",
        "    pred = tf.concat((pred, prediction_mask), axis=0)\n",
        "    inp = tf.concat((inp, label_mask), axis=0)\n",
        "\n",
        "  if step%50 == 0:\n",
        "    print('.', end='', flush=True)"
      ],
      "metadata": {
        "id": "HVwR-OzvH9Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the vectors such that labels that should be removed are not taken into consideration\n",
        "# Find the indices of the elements to remove\n",
        "def filter_vectors(tf1, tf2, index):\n",
        "  mask = tf.logical_not(tf.logical_or(tf.equal(tf1, index), tf.equal(tf2, index)))\n",
        "  indices_to_keep = tf.where(mask)[:, 0]\n",
        "\n",
        "  # Remove the elements from both tensors\n",
        "  tf1_filtered = tf.gather(tf1, indices_to_keep)\n",
        "  tf2_filtered = tf.gather(tf2, indices_to_keep)\n",
        "  return tf1_filtered, tf2_filtered"
      ],
      "metadata": {
        "id": "-syyd2TGUhJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_filtered = inp\n",
        "pred_filtered = pred\n",
        "remove_labels = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "for i in remove_labels:\n",
        "  inp_filtered, pred_filtered = filter_vectors(inp_filtered, pred_filtered, i)"
      ],
      "metadata": {
        "id": "CLd493QnE5T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un, _ = tf.unique(inp)\n",
        "un2, _ = tf.unique(pred)\n",
        "print(tf.sort(un).numpy())\n",
        "print(tf.sort(un2).numpy())"
      ],
      "metadata": {
        "id": "B6Me7BrIaNQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_mat = confusion_matrix(inp.numpy(), pred.numpy())\n",
        "total_labels = inp_filtered.shape\n",
        "confusion_mat_norm = confusion_mat/total_labels\n",
        "confusion_mat_norm[confusion_mat_norm == 0.0] = np.nan"
      ],
      "metadata": {
        "id": "rHkuZB8IKzyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfm_filtered = tf.math.confusion_matrix(inp_filtered, pred_filtered, num_classes=34)\n",
        "total_labels = inp_filtered.shape\n",
        "cfm_normalized = cfm_filtered.numpy()/total_labels\n",
        "cfm_normalized[cfm_normalized == 0.0] = np.nan\n",
        "\n",
        "#threshold = 100\n",
        "#cfm_thresholded = tf.where(cfm_filtered < threshold, 0, cfm_filtered)"
      ],
      "metadata": {
        "id": "BUxNVOxjQbGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL2qqd1ncMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vmin = 0\n",
        "vmax = np.nanmax(confusion_mat_norm)\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "heatmap = sns.heatmap(confusion_mat_norm, annot=False, cmap=\"crest\", linewidths = 0.2, linecolor = \"black\", vmin=0, vmax=vmax)\n",
        "heatmap.set(title = \"Validation dataset\", xlabel='Predicted label', ylabel='Ground truth label')\n",
        "pos_x, textvals = plt.xticks()\n",
        "pos_y, textvals = plt.yticks()\n",
        "\n",
        "plt.xticks(ticks=pos_x, labels=keep_indices.tolist())\n",
        "plt.yticks(ticks=pos_y, labels=keep_indices.tolist())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d3YC21pVDukd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_indices = tf.sort(un2).numpy()"
      ],
      "metadata": {
        "id": "-TSi7T9wLkBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_filtered.numpy().shape[0]"
      ],
      "metadata": {
        "id": "aJhTnVsgMUil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "num_labels = inp_filtered.numpy().shape[0]\n",
        "vmin = 0\n",
        "vmax = np.max(confusion_mat/num_labels)\n",
        "heatmap = sns.heatmap(confusion_mat_norm, annot=False, cmap=\"crest\", linewidths = 0.2, linecolor = \"black\")\n",
        "heatmap.set(title = \"Validation dataset\", xlabel='Predicted label', ylabel='Ground truth label')\n",
        "pos_x, textvals = plt.xticks()\n",
        "pos_y, textvals = plt.yticks()\n",
        "\n",
        "plt.xticks(ticks=pos_x, labels=valid_labels)\n",
        "plt.yticks(ticks=pos_y, labels=valid_labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yiU_GmlnWyjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPU-fOVxL4FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "heatmap = sns.heatmap(cfm_filtered, annot=False, cmap=\"crest\", linewidths = 0.2, linecolor = \"black\", norm=LogNorm())\n",
        "heatmap.set(title = \"Validation dataset\", xlabel='Predicted label', ylabel='Ground truth label')\n",
        "pos_x, textvals = plt.xticks()\n",
        "pos_y, textvals = plt.yticks()\n",
        "\n",
        "plt.xticks(ticks=pos_x, labels=tf.sort(un2).numpy().numpy().tolist())\n",
        "plt.yticks(ticks=pos_y, labels=keep_indices.numpy().tolist())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uN26byBjHk1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[row, column]\n",
        "cfm_reduced.numpy()[:,15]"
      ],
      "metadata": {
        "id": "XnWrQTxcT8bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cfm.numpy()[0,:])"
      ],
      "metadata": {
        "id": "H1fzOVriavpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cfm.numpy()[24,24])"
      ],
      "metadata": {
        "id": "XMtwB_7Ubese"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap.figure.savefig(\"sns-heatmap.png\")"
      ],
      "metadata": {
        "id": "U2wm34E9HoFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "tXJlzRexC3nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJgdDuptC4z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig('/content/heatmap.png')"
      ],
      "metadata": {
        "id": "EzkFjzqK7Glk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for masked_image, image, semantics_masked, semantics, mask, image_file, locations in tf_val.take(1)\n",
        "    semantics_sparse_test = label_to_sparse(semantics)\n",
        "    predicted_RGB = generatorSEMGAN([masked_image, semantics_sparse_test], training = True)\n",
        "    predicted_RGB = mask * predicted_RGB + (1-mask) * image\n",
        "\n",
        "    plt.imshow(predicted_RGB)"
      ],
      "metadata": {
        "id": "8QC6Ybd9cTuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZW2bZwaDoy7"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWqGQnJKDnRw"
      },
      "outputs": [],
      "source": [
        "def metrics(image_1, image_2):\n",
        "  # calculate L1 distance\n",
        "  L1_distance = tf.math.reduce_sum(tf.math.abs(image_1 - image_2))\n",
        "  ssim = tf.image.ssim(image_1, image_2, 255)\n",
        "  psnr = tf.image.psnr(image_1, image_2, 255)\n",
        "  return [L1_distance.numpy(), ssim.numpy()[0], psnr.numpy()[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def splitMaskMetrics(target, prediction, locations):\n",
        "  locations = np.squeeze(locations.numpy(), axis = 0)\n",
        "  ssim_list = []\n",
        "  psnr_list = []\n",
        "  l1_list = []\n",
        "\n",
        "  for location in locations:\n",
        "    mask_height = tf.cast(location[0]/2, tf.int32).numpy()\n",
        "    mask_width = tf.cast(location[1]/2, tf.int32).numpy()\n",
        "    height_offset = location[2]\n",
        "    width_offset = location[3]\n",
        "\n",
        "    targetMask = target[:,height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width,:]\n",
        "    predictiontMask = prediction[:, height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width, :] \n",
        "    \n",
        "    ssim = tf.image.ssim(targetMask, predictiontMask, 255)\n",
        "    psnr = tf.image.psnr(targetMask, predictiontMask, 255)\n",
        "    l1 = tf.math.reduce_sum(tf.math.abs(targetMask-predictiontMask))\n",
        "\n",
        "\n",
        "    ssim_list.append(ssim.numpy()[0])\n",
        "    psnr_list.append(psnr.numpy()[0])\n",
        "    l1_list.append(l1.numpy())\n",
        "\n",
        "  return [sum(l1_list)/len(l1_list), sum(ssim_list)/len(ssim_list), sum(psnr_list)/len(psnr_list)]"
      ],
      "metadata": {
        "id": "4e6blWhez0sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-v3EPBUCds_"
      },
      "outputs": [],
      "source": [
        "def IoU(label, masked_label):\n",
        "  _, shape_h, shape_w, _ = label.shape\n",
        "  unique_labels, _ = tf.unique(tf.reshape(label, shape_h*shape_w))\n",
        "  IoU = tf.keras.metrics.IoU(num_classes=35, target_class_ids = unique_labels)(label, masked_label)\n",
        "  return IoU.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5cPSc3k3Zds"
      },
      "outputs": [],
      "source": [
        "def get_dataset_metrics(ds):\n",
        "  #input_image is the inpainted image, GT is the groundtruth\n",
        "  names = [\"L1\",\"SSIM\",\"PSNR\"]\n",
        "  metrics_list = []\n",
        "  IoU_list = []\n",
        "  img_path_list = []\n",
        "  mask_list = []\n",
        "  img_list = []\n",
        "  semantics_list = []\n",
        "  \n",
        "  for (masked_image, image, semantics_masked, semantics, mask, image_file, locations) in ds:\n",
        "    semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True))\n",
        "    semantics_predicted = semantics_predicted*tf.cast(mask, tf.int32)+semantics*(1-tf.cast(mask, tf.int32))\n",
        "    semantics_sparse_test = label_to_sparse(semantics_predicted)\n",
        "\n",
        "    predicted_RGB = generatorSEMGAN([masked_image, semantics_sparse_test], training = True)\n",
        "    predicted_RGB = mask * predicted_RGB + (1-mask) * image\n",
        "\n",
        "    metrics_list.append(splitMaskMetrics(image, predicted_RGB, locations))\n",
        "    IoU_list.append(IoU(semantics, semantics_predicted))\n",
        "    img_path_list.append(image_file.numpy()[0].decode(\"utf-8\"))\n",
        "    img_list.append(predicted_RGB)\n",
        "    mask_list.append(mask)\n",
        "    semantics_list.append(semantics_predicted)\n",
        "  \n",
        "  metrics_df = pd.DataFrame(metrics_list)\n",
        "  metrics_df.columns = names\n",
        "  metrics_df[\"IoU\"] = IoU_list\n",
        "  metrics_df[\"Img Path\"] = img_path_list\n",
        "  metrics_df[\"Image\"] = img_list\n",
        "  metrics_df[\"Mask\"] = mask_list\n",
        "  metrics_df[\"Semantics\"] = semantics_list\n",
        "  return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(pd_dataframe, index):\n",
        "  prediction = pd_dataframe[\"Image\"][index]\n",
        "  mask = pd_dataframe[\"Mask\"][index]\n",
        "  semantics = label_to_rgb(pd_dataframe[\"Semantics\"][index])\n",
        "  gt = normalize(resize(load(pd_dataframe[\"Img Path\"][index]), 256, 512))\n",
        "  gt = tf.expand_dims(gt, 0)\n",
        "  masked = (1-mask)*gt\n",
        "  \n",
        "\n",
        "  images = [masked, gt, prediction, semantics]\n",
        "  title = [\"Input Image\", \"Ground Truth\", \"Prediction\", \"Semantics\"]\n",
        "\n",
        "  plt.figure(figsize=(25, 25))\n",
        "  for i in range(4):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(images[i][0] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "  print(\"L1: \", pd_dataframe[\"L1\"][index], \n",
        "        \"SSIM: \",pd_dataframe[\"SSIM\"][index], \n",
        "        \"PSNR: \",pd_dataframe[\"PSNR\"][index])"
      ],
      "metadata": {
        "id": "83O-LdpZweaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wTLJzBA6dKF"
      },
      "outputs": [],
      "source": [
        "metrics_test_SEMGAN = get_dataset_metrics(tf_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(metrics_test_SEMGAN, 21)"
      ],
      "metadata": {
        "id": "2gSvpuvdx-wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_test_SEMGAN[[\"L1\",\"SSIM\",\"PSNR\", \"IoU\"]]"
      ],
      "metadata": {
        "id": "bxtqnrz_qela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_test_SEMGAN.mean()"
      ],
      "metadata": {
        "id": "QxvzXkHBqiCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_test_SEMGAN.mean()"
      ],
      "metadata": {
        "id": "IQh8fpATMHJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onBvPUiWhCi0"
      },
      "outputs": [],
      "source": [
        "def reconstruct_img(label_file, mask):\n",
        "  label = tf.io.read_file(label_file)\n",
        "  label = tf.io.decode_png(label, channels = 0)\n",
        "  label = tf.cast(label, tf.int32)\n",
        "  label = resize(label, 256, 512)\n",
        "  masked_label = (1-mask)*label\n",
        "  \n",
        "\n",
        "  return label, masked_label, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVn9C-nspXkS"
      },
      "outputs": [],
      "source": [
        "def show_img(img_tuple):\n",
        "  target, input_image, mask = img_tuple\n",
        "\n",
        "  predicted = generator([input_image,mask], training=True)\n",
        "  predicted = gen_to_label(predicted)\n",
        "  predicted = predicted * mask + target*(1-mask)\n",
        "  target = tf.expand_dims(target, 0)\n",
        "  \n",
        "  #convert to rgb images\n",
        "  target = label_to_rgb(target)\n",
        "  input_image = label_to_rgb(input_image)\n",
        "  predicted = label_to_rgb(predicted)\n",
        "\n",
        "  images = [input_image[0], target[0], predicted[0]]\n",
        "  title = [\"Input Image\", \"Ground Truth\", \"Predicted Image\"]\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i, image in enumerate(images):\n",
        "    plt.subplot(3,1,i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(image*0.5+0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SemGAN"
      ],
      "metadata": {
        "id": "aZVgme-yXv40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create new dataset"
      ],
      "metadata": {
        "id": "KrX_770VNmC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(image_file, semantics_file):\n",
        "  image = load(image_file)\n",
        "  image = resize(image, IM_HEIGHT, IM_WIDTH)\n",
        "  masked_image, mask = image_mask(image, 64, 32, 4)\n",
        "  image = normalize(image)\n",
        "  masked_image = normalize(masked_image)\n",
        "  masked_image = tf.cast(masked_image, tf.float32)\n",
        "\n",
        "  mask_int = tf.cast(mask, tf.int32)\n",
        "  semantics = tf.io.read_file(semantics_file)\n",
        "  semantics = tf.io.decode_png(semantics, channels = 0)\n",
        "  semantics = tf.cast(semantics, tf.int32)\n",
        "  semantics = resize(semantics, 256, 512)\n",
        "  semantics_masked = (1-mask_int)*semantics\n",
        "  semantics_masked = tf.cast(semantics_masked, tf.float32)\n",
        "\n",
        "\n",
        "  return masked_image, image, semantics_masked, semantics, mask, image_file"
      ],
      "metadata": {
        "id": "26s6WG1INxCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill pandas dataframe with the paths to the files\n",
        "def folder_to_pd(image_dir, semantics_dir):\n",
        "  image_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "  semantics_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "\n",
        "  for data_type in [\"test\", \"train\", \"val\"]:\n",
        "    image_subdir = os.path.join(image_dir, data_type)\n",
        "    for root, subdir, files in os.walk(image_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for file in files:\n",
        "          image_paths[data_type].append(os.path.join(root, file))\n",
        "\n",
        "  for data_type in [\"test\", \"train\", \"val\"]:\n",
        "    semantics_subdir = os.path.join(semantics_dir, data_type)\n",
        "    for root, subdir, files in os.walk(semantics_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for i,file in enumerate(files):\n",
        "          if file[-12:] == \"labelIds.png\":\n",
        "            semantics_paths[data_type].append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "    \n",
        "  test_ds = pd.DataFrame(list(zip(image_paths[\"test\"], semantics_paths[\"test\"])), columns =['Images', 'Semantics'])\n",
        "  train_ds = pd.DataFrame(list(zip(image_paths[\"train\"], semantics_paths[\"train\"])), columns =['Images', 'Semantics'])\n",
        "  val_ds = pd.DataFrame(list(zip(image_paths[\"val\"], semantics_paths[\"val\"])), columns =['Images', 'Semantics'])\n",
        "\n",
        "  return test_ds, train_ds, val_ds\n",
        "\n",
        "test_pd, train_pd, val_pd = folder_to_pd(\"/content/cityScapes/img/leftImg8bit\", \"/content/cityScapes/annotations/gtFine\")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_pd[\"Images\"], test_pd[\"Semantics\"]))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_pd[\"Images\"], train_pd[\"Semantics\"]))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_pd[\"Images\"], val_pd[\"Semantics\"]))\n",
        "\n",
        "tf_train = train_ds.shuffle(train_ds.cardinality(), reshuffle_each_iteration=False)\n",
        "#tf_train = tf_train.interleave(tf.data.TFRecordDataset, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "tf_train = tf_train.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_train = tf_train.batch(BATCH_SIZE)\n",
        "tf_train = tf_train.cache(\"/content/temporary2.tfcache\")\n",
        "tf_train = tf_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "tf_val = val_ds.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_val = tf_val.batch(BATCH_SIZE)\n",
        "tf_val = tf_val.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "20dDEaOfNnzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## restore semantics GAN"
      ],
      "metadata": {
        "id": "4ykrRwDw_A9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_semantics = GeneratorSemantics()\n",
        "discriminator_semantics = DiscriminatorSemantics()"
      ],
      "metadata": {
        "id": "8aJVjpmmXzyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir_semantics = '/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting'\n",
        "checkpoint_prefix_semantics = os.path.join(checkpoint_dir_semantics, \"semantics\", \"ckpt\")\n",
        "checkpoint_semantics = tf.train.Checkpoint(generator_optimizer=generator_optimizer_semantics,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_semantics,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)"
      ],
      "metadata": {
        "id": "-FTgGifL_Lxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore('/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/semantics/ckpt-1')"
      ],
      "metadata": {
        "id": "kX4XgiEh_K_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GAN"
      ],
      "metadata": {
        "id": "Bkakhdi9_2Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator():\n",
        "        down_stack = [\n",
        "            downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 256, 64)\n",
        "            downsample(128, 4),  # (bs, 64, 128, 128)\n",
        "            downsample(256, 4),  # (bs, 32, 64, 256)\n",
        "            downsample(512, 4),  # (bs, 16, 32, 512)\n",
        "            downsample(512, 4),  # (bs, 8, 16, 512)\n",
        "            downsample(512, 4),  # (bs, 4, 8, 512)\n",
        "            downsample(512, 4),  # (bs, 2, 4, 512)\n",
        "            downsample(512, 4),  # (bs, 1, 2, 512)\n",
        "        ]\n",
        "\n",
        "        up_stack = [\n",
        "            upsample(512, 4, 1, 2, apply_dropout=True),  # (bs, 2, 4, 1024)\n",
        "            upsample(512, 4, 2, 4, apply_dropout=True),  # (bs, 4, 8, 1024)\n",
        "            upsample(512, 4, 4, 8, apply_dropout=True),  # (bs, 8, 16, 1024)\n",
        "            upsample(512, 4, 8, 16),  # (bs, 16, 32, 1024)\n",
        "            upsample(256, 4, 16, 32),  # (bs, 32, 64, 512)\n",
        "            upsample(128, 4, 32, 64),  # (bs, 64, 128, 256)\n",
        "            upsample(64, 4, 64, 128),  # (bs, 128, 256, 128)\n",
        "        ]\n",
        "\n",
        "        initializer = tf.random_normal_initializer(0., 0.02)\n",
        "        last_upsample = upsample(64, 4, 128, 256)\n",
        "        last = tf.keras.layers.Conv2D(3, 4, strides=1, padding='same', kernel_initializer=initializer, activation='tanh')  # (bs, 256, 512, 35) 35 is the number of classes \n",
        "\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        image = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 3])\n",
        "        semantics = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 35])\n",
        "\n",
        "        x = tf.keras.layers.concatenate([image, semantics])\n",
        "\n",
        "        # Downsampling through the model\n",
        "        skips = []\n",
        "        for down in down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips = reversed(skips[:-1])\n",
        "\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip in zip(up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = concat([x, skip])\n",
        "\n",
        "        x = last_upsample(x)\n",
        "        x = last(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=[image, semantics], outputs=x)"
      ],
      "metadata": {
        "id": "nchGorcV_1Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    image = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 3], name='input_image')\n",
        "    semantics = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 35], name='semantics')\n",
        "    tar = tf.keras.layers.Input(shape=[IM_HEIGHT, IM_WIDTH, 3], name='target_image')\n",
        "\n",
        "    x = tf.keras.layers.concatenate([image, semantics, tar])  # (bs, 256, 512, channels*2)\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)  # (bs, 128, 256, 64)\n",
        "    down2 = downsample(128, 4)(down1)  # (bs, 64, 128, 128)\n",
        "    down3 = downsample(256, 4)(down2)  # (bs, 32, 64, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 66, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  use_bias=False)(zero_pad1)  # (bs, 31, 63, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 65, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                  kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 62, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[image, semantics, tar], outputs=last)"
      ],
      "metadata": {
        "id": "kBfFS9XX_6k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss( disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # mean absolute error\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "pI1s_CRb_qGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generatorSEMGAN = Generator()\n",
        "discriminatorSEMGAN = Discriminator()"
      ],
      "metadata": {
        "id": "ZZLgERwCB56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generatorSEMGAN_GT = Generator()\n",
        "discriminatorSEMGAN_GT = Discriminator()"
      ],
      "metadata": {
        "id": "ri7TWg9B4yG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-4\n",
        "generator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "FZK94iRq_qQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir_SEMGAN = '/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/'\n",
        "checkpoint_prefix_SEMGAN = os.path.join(checkpoint_dir_SEMGAN, \"ckpt\", \"sparse\")\n",
        "checkpoint_SEMGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN,\n",
        "                                 discriminator=discriminatorSEMGAN)"
      ],
      "metadata": {
        "id": "cSfsSKXM_qEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore('/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/ckpt-2')"
      ],
      "metadata": {
        "id": "PzyJcfED6_yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "t12Sc33tC_AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, masked_input, tar, semantics_prediction, semantics_sparse, mask):\n",
        "  prediction = model([masked_input, semantics_sparse], training=True)\n",
        "  prediction = prediction*mask+tar*(1-mask)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  semantics_prediction = label_to_rgb(semantics_prediction)\n",
        "\n",
        "  display_list = [masked_input[0], tar[0], prediction[0], semantics_prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image', 'Input Semantics']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pd6VKzpSDZlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FIT SEMGAN"
      ],
      "metadata": {
        "id": "5f0GQfhA4cVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epochSEMGAN(input_image, target, semantics, mask):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "    disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generatorSEMGAN.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminatorSEMGAN.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generatorSEMGAN.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminatorSEMGAN.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLossSEMGAN(input_image, target, semantics, mask): \n",
        "  gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "  disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss"
      ],
      "metadata": {
        "id": "7l0qEzLCC-Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/SEMGAN/epoch/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Train\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Val\")"
      ],
      "metadata": {
        "id": "IyhePSwnDIKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitSEMGAN(train_ds, test_ds, epochs):\n",
        "  masked_image_test, image_test, semantics_masked_test, semantics_test, mask_test, image_file_test = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    \n",
        "    #semantics used for the example\n",
        "    semantics_predicted_test = gen_to_label(generator_semantics(semantics_masked_test, training=True)) # turn the generator output into labels\n",
        "    semantics_predicted_test = semantics_predicted_test*tf.cast(mask_test, tf.int32)+semantics_test*(1-tf.cast(mask_test, tf.int32)) # combine the prediction with the ground truth\n",
        "    semantics_sparse_test = label_to_sparse(semantics_predicted_test) # make a sparse matrix as input to the generator\n",
        "    generate_images(generatorSEMGAN, masked_image_test, image_test, semantics_predicted_test, semantics_sparse_test, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(masked_image, image, semantics_masked, semantics, mask, image_file) in train_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True)) # turn the generator output into labels\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32)) # combine the prediction with the ground truth\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted) # make a sparse matrix as input to the generator\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epochSEMGAN(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict[\"gen_L1_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(masked_image, image, semantics_masked, semantics, mask, image_file) in test_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True))\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32))\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted)\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = getValAccLossSEMGAN(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict_val[\"gen_L1_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "    #if epoch % 10 == 0 & epoch !=0: \n",
        "    #  checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "52I6JOc9DBgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitSEMGAN(tf_train, tf_val, 10)"
      ],
      "metadata": {
        "id": "f44ubeTy4ZKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FIT SEMGAN-GT"
      ],
      "metadata": {
        "id": "C2YgnXeR4hrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epochSEMGAN_GT(input_image, target, semantics, mask):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generatorSEMGAN_GT([input_image, semantics], training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminatorSEMGAN_GT([input_image, semantics, target], training=True)\n",
        "    disc_generated_output = discriminatorSEMGAN_GT([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generatorSEMGAN_GT.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminatorSEMGAN_GT.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generatorSEMGAN_GT.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminatorSEMGAN_GT.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLossSEMGAN_GT(input_image, target, semantics, mask): \n",
        "  gen_output = generatorSEMGAN_GT([input_image, semantics], training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminatorSEMGAN_GT([input_image, semantics, target], training=True)\n",
        "  disc_generated_output = discriminatorSEMGAN_GT([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss"
      ],
      "metadata": {
        "id": "XJv4CUns33iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitSEMGAN_GT(train_ds, test_ds, epochs):\n",
        "  masked_image_test, image_test, semantics_masked_test, semantics_test, mask_test, image_file_test = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    \n",
        "    #semantics used for the example\n",
        "    semantics_predicted_test = gen_to_label(generator_semantics(semantics_masked_test, training=True)) # turn the generator output into labels\n",
        "    semantics_predicted_test = semantics_predicted_test*tf.cast(mask_test, tf.int32)+semantics_test*(1-tf.cast(mask_test, tf.int32)) # combine the prediction with the ground truth\n",
        "    semantics_sparse_test = label_to_sparse(semantics_predicted_test) # make a sparse matrix as input to the generator\n",
        "    generate_images(generatorSEMGAN_GT, masked_image_test, image_test, semantics_predicted_test, semantics_sparse_test, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(masked_image, image, semantics_masked, semantics, mask, image_file) in train_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True)) # turn the generator output into labels\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32)) # combine the prediction with the ground truth\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted) # make a sparse matrix as input to the generator\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epochSEMGAN_GT(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "      \n",
        "      for i,(key,value) in enumerate(metrics_dict.items()):\n",
        "        metrics_dict[key] = value + metrics_list[i]\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    with summary_writer_train.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict[\"gen_total_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict[\"gen_gan_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict[\"gen_L1_loss\"]/total_ims, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict[\"disc_loss\"]/total_ims, step=epoch)\n",
        "\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(masked_image, image, semantics_masked, semantics, mask, image_file) in test_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True))\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32))\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted)\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = getValAccLossSEMGAN_GT(masked_image, image, semantics_predicted, mask)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "        \n",
        "      for i,(key,value) in enumerate(metrics_dict_val.items()):\n",
        "        metrics_dict_val[key] = value + metrics_list_val[i]\n",
        "    \n",
        "    with summary_writer_val.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', metrics_dict_val[\"gen_total_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_gan_loss', metrics_dict_val[\"gen_gan_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('gen_L1_loss', metrics_dict_val[\"gen_L1_loss\"]/total_ims_val, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', metrics_dict_val[\"disc_loss\"]/total_ims_val, step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()\n",
        "\n",
        "    # Save (checkpoint) the model every 5 epochs\n",
        "    #if epoch % 10 == 0 & epoch !=0: \n",
        "    #  checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "wc1EZtV73tfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitSEMGAN_GT(tf_train, tf_val, 10)"
      ],
      "metadata": {
        "id": "tVBJuKFKGOrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(checkpoint_prefix_SEMGAN)"
      ],
      "metadata": {
        "id": "uFsOFHew_lYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.save(file_prefix=checkpoint_prefix_SEMGAN)"
      ],
      "metadata": {
        "id": "n_ID1USi_mQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-OgBpEXVk5uq",
        "0XmDPCTFV8jk",
        "nLrCbfBgwcDR",
        "Sst0fJYuk2CU",
        "8HpF51xNho5s",
        "k__WK3vcNtTE",
        "AIUi-Ngv1Mpe"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}