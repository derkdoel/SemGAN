{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2Qcr2R4c9DMY",
        "PNlSKlBUKlbi",
        "Up9HfI1T9iUP"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNqpqVs9g3kA"
      },
      "source": [
        "#Import cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJyZR9Q351CE"
      },
      "outputs": [],
      "source": [
        "! mkdir /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkYja-9MWqdX"
      },
      "outputs": [],
      "source": [
        "! wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=d.a.vandendoel@students.uu.nl&password=Derkojo95!&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1 -P /content/cityScapes\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3 -P /content/cityScapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ps2MJcC6K49"
      },
      "outputs": [],
      "source": [
        "! unzip /content/cityScapes/gtFine_trainvaltest.zip -d /content/cityScapes/annotations\n",
        "! unzip /content/cityScapes/leftImg8bit_trainvaltest.zip -d /content/cityScapes/img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGT_vwW0hJRK"
      },
      "source": [
        "#Mount drive and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waujOpX8RF0k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufsj9SjJsEFT"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install cityscapesscripts"
      ],
      "metadata": {
        "id": "BC3rkgDtoq6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Scriptie/python\n",
        "import Generators\n",
        "import Discriminators"
      ],
      "metadata": {
        "id": "CVqeiaDaCD_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIJTdDilt6-Y"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "import cv2\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "import PIL\n",
        "import scipy\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "\n",
        "from cityscapesscripts.preparation.json2instanceImg import json2instanceImg\n",
        "from cityscapesscripts.preparation.json2labelImg import json2labelImg\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import SpectralNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "from tqdm import tqdm\n",
        "from random import randint, seed\n",
        "from google.colab.patches import cv2_imshow\n",
        "from imageio import imread\n",
        "from tensorflow.keras import layers\n",
        "from glob import glob\n",
        "from IPython import display\n",
        "from random import randint\n",
        "from skimage.metrics import structural_similarity as SSIM\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "print(os.listdir(\"../content/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qcr2R4c9DMY"
      },
      "source": [
        "# Load image and apply mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN_KUKw4-pt6"
      },
      "outputs": [],
      "source": [
        "TRAIN = \"/content/drive/MyDrive/Scriptie/img/train/*.png\"\n",
        "TEST = \"/content/drive/MyDrive/Scriptie/img/test/*.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbtkLyLDqs4l"
      },
      "outputs": [],
      "source": [
        "IM_HEIGHT = 256\n",
        "IM_WIDTH = 512\n",
        "BUFFER_SIZE = 2975\n",
        "BATCH_SIZE = 1\n",
        "OUTPUT_CHANNELS = 3\n",
        "MASK_HEIGTH = 64\n",
        "MASK_WIDTH = 32\n",
        "LAMBDA = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cz4Oiv0JndM"
      },
      "outputs": [],
      "source": [
        "def load(image_file, channels, semantics = False,):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.io.decode_png(image, channels = channels)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtY1dgp7aSIn"
      },
      "outputs": [],
      "source": [
        "def resize(input_image, height, width):\n",
        "    image = tf.image.resize(\n",
        "        input_image, \n",
        "        [height, width],\n",
        "        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
        "    )\n",
        "  \n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG5mJYAlBjlK"
      },
      "outputs": [],
      "source": [
        "def normalize(image):\n",
        "  image = image / 127.5 - 1\n",
        "  return image\n",
        "\n",
        "# Normalize values between 0 and 1 for image display\n",
        "def denormalize(image):\n",
        "  return image *0.5+0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVdxoGt_EYj6"
      },
      "outputs": [],
      "source": [
        "def image_mask(image, mask_height, mask_width, num_masks, rand_seed = None):\n",
        "  # random_cutout function needs mask_height * mask_width to be divisible by 2\n",
        "  if (mask_height * mask_width)%2 != 0:\n",
        "    raise Exception(\"Error! mask size must be divisible by 2\")\n",
        "  \n",
        "  im_height = tf.cast(IM_HEIGHT, dtype=tf.dtypes.int32)\n",
        "  im_width = tf.cast(IM_WIDTH, dtype=tf.dtypes.int32)  \n",
        "  mask_height = tf.cast(mask_height, dtype=tf.dtypes.int32)\n",
        "  mask_width = tf.cast(mask_width, dtype=tf.dtypes.int32)\n",
        "  \n",
        "  min_height = tf.cast(tf.math.round((mask_height/2)+15), tf.dtypes.int32)\n",
        "  min_width = tf.cast(tf.math.round((mask_width/2)+10), tf.dtypes.int32)\n",
        "  max_height = tf.cast(tf.math.round(im_height/10), tf.dtypes.int32)\n",
        "\n",
        "  try:\n",
        "    image_shape = image.shape\n",
        "  except:\n",
        "    image_shape = image.get_shape()\n",
        "\n",
        "  if len(image_shape) == 3:\n",
        "    image = tf.expand_dims(image, 0)\n",
        "  elif len(image_shape) == 4:\n",
        "    pass\n",
        "  else:\n",
        "    raise Exception('Error! Tensor shape must either be: (batch_size, image height, image width, color channels) or (image height, image width, color channels)')\n",
        "  \n",
        "  mask = tf.zeros_like(image, tf.dtypes.float32)[:,:,:,0:1]\n",
        "  locations = []\n",
        "\n",
        "  #create masks\n",
        "  for i in range(num_masks):\n",
        "    mask_height_random = tf.random.uniform(shape = (), minval= mask_height-5, maxval=mask_height+15, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask_height_random = mask_height_random if mask_height_random%2 == 0 else mask_height_random-1\n",
        "    mask_width_random = tf.random.uniform(shape = (), minval= mask_width, maxval=mask_width+10, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask_width_random = mask_width_random if mask_width_random%2 == 0 else mask_width_random-1\n",
        "    \n",
        "    height_offset = tf.random.uniform(shape = (), minval= min_height, maxval=im_height-max_height, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    width_offset = tf.random.uniform(shape = (), minval= min_width, maxval= im_width-min_width, dtype=tf.dtypes.int32, seed=rand_seed)\n",
        "    mask = tfa.image.cutout(mask, (mask_height_random, mask_width_random), (height_offset, width_offset), constant_values = 1)\n",
        "    locations.append((mask_height_random, mask_width_random, height_offset, width_offset))\n",
        "  \n",
        "  masked_image = (1-mask) * image\n",
        "\n",
        "    #mask = tf.zeros([image_heigt, image_width])\n",
        "    #mask = mask[height_offset:height_offset+mask_height].assign(tf.ones([mask_height, mask_width]))\n",
        "    #image = mask * image\n",
        "\n",
        "\n",
        "  return masked_image[0], mask[0], locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgw3eWvVsxVH"
      },
      "outputs": [],
      "source": [
        "def load_images(image_file, semantics_file, json_file):\n",
        "  image = load(image_file, 3)\n",
        "  image = resize(image, IM_HEIGHT, IM_WIDTH)\n",
        "  masked_image, mask, locations = image_mask(image, 64, 32, 4)\n",
        "  image = normalize(image)\n",
        "  masked_image = normalize(masked_image)\n",
        "\n",
        "  semantics = tf.io.read_file(json_file)\n",
        "  semantics = tf.io.decode_png(semantics, channels = 0)\n",
        "  semantics = tf.cast(semantics, tf.int32)\n",
        "  semantics = resize(semantics, 256, 512)\n",
        "  masked_semantics = semantics * (1-tf.cast(mask, tf.int32))\n",
        "\n",
        "  return image, masked_image, semantics, masked_semantics, mask, locations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training dataset"
      ],
      "metadata": {
        "id": "PNlSKlBUKlbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill pandas dataframe with the paths to the files\n",
        "def folder_to_pd(image_dir, semantics_dir):\n",
        "  image_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "  semantics_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "  json_paths = {\"test\": [], \"train\": [], \"val\": []}\n",
        "\n",
        "  for data_type in [\"test\", \"train\", \"val\"]:\n",
        "    image_subdir = os.path.join(image_dir, data_type)\n",
        "    for root, subdir, files in os.walk(image_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for file in files:\n",
        "          image_paths[data_type].append(os.path.join(root, file))\n",
        "\n",
        "  \n",
        "  for data_type in [\"test\", \"train\", \"val\"]:\n",
        "    semantics_subdir = os.path.join(semantics_dir, data_type)\n",
        "    for root, subdir, files in os.walk(semantics_subdir):\n",
        "      subdir.sort()\n",
        "      files.sort()\n",
        "      if files:\n",
        "        for i,file in enumerate(files):\n",
        "          if file[-9:] == \"color.png\":\n",
        "            semantics_paths[data_type].append(os.path.join(root, file))\n",
        "          if file[-12:] == \"labelIds.png\":\n",
        "            json_paths[data_type].append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "  test_ds = pd.DataFrame(list(zip(image_paths[\"test\"], semantics_paths[\"test\"], json_paths[\"test\"])), columns =['Image', 'Semantics', 'label'])\n",
        "  train_ds = pd.DataFrame(list(zip(image_paths[\"train\"], semantics_paths[\"train\"], json_paths[\"train\"])), columns =['Image', 'Semantics', 'label'])\n",
        "  val_ds = pd.DataFrame(list(zip(image_paths[\"val\"], semantics_paths[\"val\"], json_paths[\"val\"])), columns =['Image', 'Semantics', 'label'])\n",
        "\n",
        "  return test_ds, train_ds, val_ds\n",
        "\n",
        "test_pd, train_pd, val_pd = folder_to_pd(\"/content/cityScapes/img/leftImg8bit\", \"/content/cityScapes/annotations/gtFine\")\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_pd[\"Image\"], test_pd[\"Semantics\"], test_pd[\"label\"]))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_pd[\"Image\"], train_pd[\"Semantics\"], train_pd[\"label\"]))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_pd[\"Image\"], val_pd[\"Semantics\"], val_pd[\"label\"]))"
      ],
      "metadata": {
        "id": "ERzIsuXD7d5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train = train_ds.shuffle(train_ds.cardinality(), reshuffle_each_iteration=True)\n",
        "tf_train = tf_train.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_train = tf_train.batch(BATCH_SIZE)\n",
        "#tf_train = tf_train.cache(\"/content/temporary.tfcache\")\n",
        "tf_train = tf_train.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "CXzJBI8B70yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_val = val_ds.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "tf_val = tf_val.batch(BATCH_SIZE)\n",
        "tf_val = tf_val.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jsXYTKvO700b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up9HfI1T9iUP"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generators.Generator(3)\n",
        "discriminator = Discriminators.Discriminator()"
      ],
      "metadata": {
        "id": "p1sDIvVJgjSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss( disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # mean absolute error\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "cTSsCGeQe4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "sTFne02efJiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the optimzer for the task of fine-tuning to get the best samples."
      ],
      "metadata": {
        "id": "U53HKKD0f3Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)"
      ],
      "metadata": {
        "id": "I265uMWXfEFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Scriptie/ckpt/GAN/'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_2e-5\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "zkQTKAEbfGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, test_input, tar, mask):  \n",
        "  prediction = model(test_input, training=True)\n",
        "  prediction = prediction*mask+tar*(1-mask)\n",
        "  plt.figure(figsize=(20, 20))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(denormalize(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7V0Y6CutfI-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epoch(input_image, mask, target):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLoss(input_image, mask, target): \n",
        "  gen_output = generator(input_image, training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) \n",
        "\n",
        "  disc_real_output = discriminator([input_image, target], training=True)\n",
        "  disc_generated_output = discriminator([input_image, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_CE_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_CE_loss, disc_loss"
      ],
      "metadata": {
        "id": "kNbg_FTqfQ88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/finetune/GAN/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_TrainFT\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_ValFT\")"
      ],
      "metadata": {
        "id": "FV9dSWdkfRTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_ds, test_ds, epochs):\n",
        "  #masked_image, image, semantics, masked_semantics, mask, locations\n",
        "  example_target, example_input, label_test, label_masked_test, mask_test, locations = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    metrics_dict = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    generate_images(generator, example_input, example_target, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(target, input_image, labels, masked_labels, mask, locations) in train_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epoch(input_image, mask, target)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "\n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    # Gather the metrics for the validation set\n",
        "    metrics_dict_val = {\"gen_total_loss\":0, \"gen_gan_loss\":0, \"gen_L1_loss\":0, \"disc_loss\":0}\n",
        "    print(\"Gathering validation set metrics...\")\n",
        "    for step,(target, input_image, labels, masked_labels, mask, locations) in test_ds.enumerate():\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = getValAccLoss(input_image, mask, target)\n",
        "      metrics_list_val = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()"
      ],
      "metadata": {
        "id": "O83MycNUfRWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(tf_train, tf_val, 10)"
      ],
      "metadata": {
        "id": "-PyLhp7-RJOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training SemGAN"
      ],
      "metadata": {
        "id": "1q_3628CUNR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SemanticsGAN"
      ],
      "metadata": {
        "id": "CLUPcnoecq5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (246, 35,232) ), #244,35,232\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ), #107 142 35\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]\n",
        "\n",
        "#convert rgb to grayscale values, this produces a unique grayscale value for each label\n",
        "def rgb_gray(rgb):\n",
        "  r, g ,b = rgb\n",
        "  return 0.2989*r + 0.5870*g + 0.1140*b\n",
        "\n",
        "#create dictionary with grayscale as key and id as value\n",
        "color_id_list = []\n",
        "color_id = {}\n",
        "id_r = {}\n",
        "id_g = {}\n",
        "id_b = {}\n",
        "for label in labels:\n",
        "  key_list = tf.image.rgb_to_grayscale(tf.convert_to_tensor(label.color))\n",
        "  key = int(rgb_gray(label.color))\n",
        "  color_id_list.append((key_list, label.id))\n",
        "  color_id[key] = label.id\n",
        "  id_r[label.id] = label.color[0]\n",
        "  id_g[label.id] = label.color[1]\n",
        "  id_b[label.id] = label.color[2]\n",
        "color_id[0] = 0\n",
        "color_id[16] = 26 # grayscale value 16 has category \"car\"\n",
        "\n",
        "\n",
        "#tf.image.rgb_to_grayscale\n",
        "#convert dictionary to tf StaticHashTable\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(color_id.keys())), tf.convert_to_tensor((list(color_id.values()))))\n",
        "gray2label = tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_r.keys())), tf.convert_to_tensor((list(id_r.values()))))\n",
        "label2r =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_g.keys())), tf.convert_to_tensor((list(id_g.values()))))\n",
        "label2g =  tf.lookup.StaticHashTable(init,default_value= -100)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(tf.convert_to_tensor(list(id_b.keys())), tf.convert_to_tensor((list(id_b.values()))))\n",
        "label2b =  tf.lookup.StaticHashTable(init,default_value= -100)"
      ],
      "metadata": {
        "id": "52rKSHZGUQuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_rgb(labeled_im):\n",
        "  r = label2r.lookup(labeled_im)[:,:,:,0]\n",
        "  g = label2g.lookup(labeled_im)[:,:,:,0]\n",
        "  b = label2b.lookup(labeled_im)[:,:,:,0]\n",
        "\n",
        "  image = tf.stack([r,g,b], axis = 3)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = normalize(image)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "n8b2pHOHdSEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_to_label(gen_output):\n",
        "  return tf.expand_dims(tf.math.argmax(gen_output, axis = -1, output_type = tf.dtypes.int32), -1)\n",
        "\n",
        "def label_to_sparse(labels):\n",
        "  return tf.squeeze(tf.one_hot(labels, depth = 35), axis=3)"
      ],
      "metadata": {
        "id": "jVC1LJ1wGFow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  # Sparse Categorical Crossentropy\n",
        "  CE_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(target, gen_output)\n",
        "  total_gen_loss = gan_loss + CE_loss\n",
        "\n",
        "  return total_gen_loss, gan_loss, CE_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "metadata": {
        "id": "-3E5dkQxrgNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_semantics = Generators.GeneratorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "discriminator_semantics = Discriminators.DiscriminatorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "lr = 2e-4\n",
        "generator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer_semantics = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "tx9xIeoZdWyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"semantics\", \"ckpt\")\n",
        "checkpoint_semantics = tf.train.Checkpoint(generator_optimizer=generator_optimizer_semantics,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_semantics,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)"
      ],
      "metadata": {
        "id": "dW-piaz9dass"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_semantics.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/semantics/ckpt-1\")"
      ],
      "metadata": {
        "id": "ceiwtIBwAodY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SemGAN"
      ],
      "metadata": {
        "id": "MAuYZQaQd1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generatorSEMGAN = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)"
      ],
      "metadata": {
        "id": "8neikwRLeDxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-4\n",
        "generator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "iJ10AD4EeD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir_SEMGAN = '/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/SEMGAN'\n",
        "checkpoint_prefix_SEMGAN = os.path.join(checkpoint_dir_SEMGAN, \"SemGAN\")\n",
        "checkpoint_SEMGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN,\n",
        "                                 discriminator=discriminatorSEMGAN)"
      ],
      "metadata": {
        "id": "ipBICX2teJLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training SemGAN"
      ],
      "metadata": {
        "id": "obLbfO4beNcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "def generator_loss( disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # mean absolute error\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "W1smr4t4eArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, masked_input, tar, semantics_prediction, semantics_sparse, mask):\n",
        "  prediction = model([masked_input, semantics_sparse], training=True)\n",
        "  prediction = prediction*mask+tar*(1-mask)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  semantics_prediction = label_to_rgb(semantics_prediction)\n",
        "\n",
        "  display_list = [masked_input[0], tar[0], prediction[0], semantics_prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image', 'Input Semantics']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "u2ol-bSXePee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epochSEMGAN(input_image, target, semantics, mask):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "    disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generatorSEMGAN.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminatorSEMGAN.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generatorSEMGAN.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminatorSEMGAN.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLossSEMGAN(input_image, target, semantics, mask): \n",
        "  gen_output = generatorSEMGAN([input_image, semantics], training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminatorSEMGAN([input_image, semantics, target], training=True)\n",
        "  disc_generated_output = discriminatorSEMGAN([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss"
      ],
      "metadata": {
        "id": "BveAWFclePhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir= '/content/drive/MyDrive/Scriptie/logs/finetune/SemGAN/'\n",
        "summary_writer_train = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Train\")\n",
        "summary_writer_val = tf.summary.create_file_writer(log_dir + \"fit/combined/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_Val\")"
      ],
      "metadata": {
        "id": "iSyn2LJZePjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitSEMGAN(train_ds, test_ds, epochs):\n",
        "  image_test, masked_image_test, semantics_test, semantics_masked_test, mask_test, locations_test = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):    \n",
        "    #semantics used for the example\n",
        "    semantics_predicted_test = gen_to_label(generator_semantics(semantics_masked_test, training=True)) # turn the generator output into labels\n",
        "    semantics_predicted_test = semantics_predicted_test*tf.cast(mask_test, tf.int32)+semantics_test*(1-tf.cast(mask_test, tf.int32)) # combine the prediction with the ground truth\n",
        "    semantics_sparse_test = label_to_sparse(semantics_predicted_test) # make a sparse matrix as input to the generator\n",
        "\n",
        "    generate_images(generatorSEMGAN, masked_image_test, image_test, semantics_predicted_test, semantics_sparse_test, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(image, masked_image, semantics, semantics_masked, mask, locations) in train_ds.enumerate():\n",
        "      semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training=True)) # turn the generator output into labels\n",
        "      semantics_predicted = semantics_predicted * tf.cast(mask, tf.int32) + semantics *(1-tf.cast(mask, tf.int32)) # combine the prediction with the ground truth\n",
        "      semantics_predicted = label_to_sparse(semantics_predicted) # make a sparse matrix as input to the generator\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epochSEMGAN(masked_image, image, semantics_predicted, mask)\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)      \n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()"
      ],
      "metadata": {
        "id": "pKltqV__eTDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitSEMGAN(tf_train, tf_val, 10)"
      ],
      "metadata": {
        "id": "W6t6T_NFftLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_SEMGAN.save(checkpoint_prefix_SEMGAN)"
      ],
      "metadata": {
        "id": "n8dBl292XO4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEMGAN-GT"
      ],
      "metadata": {
        "id": "81SkTlL2DVFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generatorSEMGAN_GT = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN_GT = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)"
      ],
      "metadata": {
        "id": "LQUMSVVLC-j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-4\n",
        "generator_optimizer_GT = tf.keras.optimizers.Adam(lr, beta_1=0.9)\n",
        "discriminator_optimizer_GT = tf.keras.optimizers.Adam(lr, beta_1=0.9)"
      ],
      "metadata": {
        "id": "UKRFlqBQDc7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir_SEMGAN_GT = '/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/SEMGAN_GT'\n",
        "checkpoint_prefix_SEMGAN_GT = os.path.join(checkpoint_dir_SEMGAN_GT, \"SemGAN_GT\")\n",
        "checkpoint_SEMGAN_GT = tf.train.Checkpoint(generator_optimizer=generator_optimizer_GT,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_GT,\n",
        "                                 generator=generatorSEMGAN_GT,\n",
        "                                 discriminator=discriminatorSEMGAN_GT)"
      ],
      "metadata": {
        "id": "dRZx4pu1Dc9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training SemGAN-GT"
      ],
      "metadata": {
        "id": "yX1oqy3sBK5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_epochSEMGAN_GT(input_image, target, semantics, mask):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generatorSEMGAN_GT([input_image, semantics], training=True)\n",
        "    gen_output = gen_output*mask+ target*(1-mask) #combine generated patches with valid pixels from ground truth\n",
        "\n",
        "    disc_real_output = discriminatorSEMGAN_GT([input_image, semantics, target], training=True)\n",
        "    disc_generated_output = discriminatorSEMGAN_GT([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generatorSEMGAN_GT.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                              discriminatorSEMGAN_GT.trainable_variables)\n",
        "\n",
        "  generator_optimizer_GT.apply_gradients(zip(generator_gradients, generatorSEMGAN_GT.trainable_variables))\n",
        "  discriminator_optimizer_GT.apply_gradients(zip(discriminator_gradients, discriminatorSEMGAN_GT.trainable_variables))\n",
        "  \n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss\n",
        "    \n",
        "  \n",
        "def getValAccLossSEMGAN_GT(input_image, target, semantics, mask): \n",
        "  gen_output = generatorSEMGAN_GT([input_image, semantics], training=True)\n",
        "  gen_output = gen_output*mask+ target*(1-mask) #convert the 35 channel output of the generator to a 1 channel output with labels\n",
        "\n",
        "  disc_real_output = discriminatorSEMGAN_GT([input_image, semantics, target], training=True)\n",
        "  disc_generated_output = discriminatorSEMGAN_GT([input_image, semantics, gen_output], training=True) #discriminator takes predicted labels\n",
        "\n",
        "  gen_total_loss, gen_gan_loss, gen_L1_loss = generator_loss(disc_generated_output, gen_output, target) #CE loss of 35 channel output\n",
        "  disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "\n",
        "  return gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss"
      ],
      "metadata": {
        "id": "hVIssTclBPyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitSEMGAN_GT(train_ds, test_ds, epochs):\n",
        "  image_test, masked_image_test, semantics_test, semantics_masked_test, mask_test, locations = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "  total_ims = tf.cast(train_ds.cardinality(), tf.float32)\n",
        "  total_ims_val = tf.cast(test_ds.cardinality(), tf.float32)\n",
        "\n",
        "  for epoch in range(epochs):    \n",
        "    semantics_sparse_test = label_to_sparse(semantics_test)\n",
        "    generate_images(generatorSEMGAN_GT, masked_image_test, image_test, semantics_test, semantics_sparse_test, mask_test)\n",
        "\n",
        "    print(\"Start training of epoch: \", epoch+1)\n",
        "    for step,(image, masked_image, semantics, semantics_masked, mask, locations) in train_ds.enumerate():\n",
        "      semantics = label_to_sparse(semantics)\n",
        "      gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss = train_epochSEMGAN_GT(masked_image, image, semantics, mask)\n",
        "      metrics_list = (gen_total_loss, gen_gan_loss, gen_L1_loss, disc_loss)\n",
        "      \n",
        "      if (step+1) % 50 == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "      \n",
        "    display.clear_output(wait=True)\n",
        "    print(f'Time taken for 1 epoch: {time.time()-start:.2f} sec\\n')\n",
        "    start = time.time()"
      ],
      "metadata": {
        "id": "2p41O7j4BQar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitSEMGAN_GT(tf_train, tf_val, 10)"
      ],
      "metadata": {
        "id": "scHCV-CEiCfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_SEMGAN_GT.save(checkpoint_prefix_SEMGAN_GT)"
      ],
      "metadata": {
        "id": "HPMLJDGYiCnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparison"
      ],
      "metadata": {
        "id": "3DyrNgAB-pHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generators.Generator(OUTPUT_CHANNELS)\n",
        "discriminator = Discriminators.Discriminator()\n",
        "\n",
        "generator_semantics = Generators.GeneratorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "discriminator_semantics = Discriminators.DiscriminatorSemantics(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "generatorSEMGAN = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)\n",
        "\n",
        "generatorSEMGAN_GT = Generators.GeneratorSemGAN(OUTPUT_CHANNELS, IM_HEIGHT, IM_WIDTH)\n",
        "discriminatorSEMGAN_GT = Discriminators.DiscriminatorSemGAN(IM_HEIGHT, IM_WIDTH)"
      ],
      "metadata": {
        "id": "PJ1lDACU-uNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-5\n",
        "generator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "\n",
        "generator_optimizer_legacy = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)\n",
        "discriminator_optimizer_legacy = tf.keras.optimizers.legacy.Adam(lr, beta_1=0.5)"
      ],
      "metadata": {
        "id": "dfnsehYvdD28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "checkpointSem = tf.train.Checkpoint(generator_optimizer=generator_optimizer_legacy,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_legacy,\n",
        "                                 generator=generator_semantics,\n",
        "                                 discriminator=discriminator_semantics)\n",
        "\n",
        "checkpointSEMGAN = tf.train.Checkpoint(generator_optimizer=generator_optimizer_legacy,\n",
        "                                 discriminator_optimizer=discriminator_optimizer_legacy,\n",
        "                                 generator=generatorSEMGAN,\n",
        "                                 discriminator=discriminatorSEMGAN)\n",
        "\n",
        "checkpointSEMGAN_GT = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorSEMGAN_GT,\n",
        "                                 discriminator=discriminatorSEMGAN_GT)\n",
        "\n",
        "checkpointGAN.restore(\"/content/drive/MyDrive/Scriptie/ckpt/GAN/ckpt_2e-5-1\")\n",
        "checkpointSem.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SemanticInpainting/semantics/ckpt-1\")\n",
        "checkpointSEMGAN.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/SEMGAN/SemGAN-1\")\n",
        "checkpointSEMGAN_GT.restore(\"/content/drive/MyDrive/Scriptie/ckpt/SEMGAN/SEMGAN_GT/SemGAN_GT-1\")"
      ],
      "metadata": {
        "id": "wodtnaq1JOQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "mxBdC9TDgF5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the metrics of the dataset. The output of *load_images_cs()* is: *image, masked_image, labels, masked_labels, mask*"
      ],
      "metadata": {
        "id": "ogARt2WogHOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(image_1, image_2):\n",
        "  # calculate L1 distance\n",
        "  L1_distance = tf.math.reduce_sum(tf.math.abs(image_1 - image_2))\n",
        "  ssim = tf.image.ssim(image_1, image_2, 2.0)\n",
        "  psnr = tf.image.psnr(image_1, image_2, 2.0)\n",
        "  return [L1_distance.numpy(), ssim.numpy()[0], psnr.numpy()[0]]"
      ],
      "metadata": {
        "id": "7hWQ2M-IgFYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitMaskMetrics(target, prediction, locations):\n",
        "    locations = np.squeeze(locations.numpy(), axis = 0)\n",
        "    ssim_list = []\n",
        "    psnr_list = []\n",
        "\n",
        "    for location in locations:\n",
        "        mask_height = tf.cast(location[0]/2, tf.int32).numpy()\n",
        "        mask_width = tf.cast(location[1]/2, tf.int32).numpy()\n",
        "        height_offset = location[2]\n",
        "        width_offset = location[3]\n",
        "\n",
        "        targetMask = target[:, height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width,:]\n",
        "        predictiontMask = prediction[:, height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width, :] \n",
        "\n",
        "        ssim = tf.image.ssim(targetMask, predictiontMask, 2.0)\n",
        "        psnr = tf.image.psnr(targetMask, predictiontMask, 2.0)\n",
        "\n",
        "\n",
        "        ssim_list.append(ssim.numpy()[0])\n",
        "        psnr_list.append(psnr.numpy()[0])\n",
        "    \n",
        "\n",
        "    l1 = tf.math.reduce_sum(tf.math.abs(target-prediction))\n",
        "\n",
        "    return [l1.numpy(), sum(ssim_list)/len(ssim_list), sum(psnr_list)/len(psnr_list)]"
      ],
      "metadata": {
        "id": "ladR7VqW59gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getIoU(label, masked_label, locations):\n",
        "  locations = np.squeeze(locations.numpy(), axis = 0)\n",
        "  _, shape_h, shape_w, _ = label.shape\n",
        "  unique_labels, _ = tf.unique(tf.reshape(label, shape_h*shape_w))\n",
        "  IoU_list = []\n",
        "\n",
        "  for location in locations:\n",
        "        mask_height = tf.cast(location[0]/2, tf.int32).numpy()\n",
        "        mask_width = tf.cast(location[1]/2, tf.int32).numpy()\n",
        "        height_offset = location[2]\n",
        "        width_offset = location[3]\n",
        "\n",
        "        targetMask = label[:, height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width,:]\n",
        "        predictiontMask = masked_label[:, height_offset-mask_height:height_offset+mask_height, width_offset-mask_width:width_offset+mask_width, :] \n",
        "        IoU = tf.keras.metrics.IoU(num_classes=35, target_class_ids = unique_labels)(targetMask, predictiontMask)\n",
        "        IoU_list.append(IoU.numpy())\n",
        "  \n",
        "  return sum(IoU_list)/len(IoU_list)"
      ],
      "metadata": {
        "id": "dwZusDwkrSYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_metrics(ds):\n",
        "  #input_image is the inpainted image, GT is the groundtruth\n",
        "  names = [\"L1\",\"SSIM\",\"PSNR\"]\n",
        "  metrics_list_GAN = []\n",
        "  metrics_list_SEMGAN = []\n",
        "  metrics_list_SEMGAN_GT = []\n",
        "  mask_list = []\n",
        "  GT_list = []\n",
        "  IoU_list = []\n",
        "  location_list = []\n",
        "  path = \"/content/predictions\"\n",
        "\n",
        "  for step, (image, masked_image, semantics, semantics_masked, mask, locations) in enumerate(ds):\n",
        "    display.clear_output(wait=True)\n",
        "    prediction = generator(masked_image, training = True)\n",
        "    prediction = prediction * mask + image*(1-mask)\n",
        "    target_path_im = os.path.join(path, \"GAN\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "    \n",
        "    metrics_list_GAN.append(splitMaskMetrics(image, prediction, locations))\n",
        "    GT_list.append(image)\n",
        "    mask_list.append(mask)\n",
        "    location_list.append(locations)\n",
        "\n",
        "    semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training = True))\n",
        "    mask_int =  tf.cast(mask, tf.int32)\n",
        "    semantics_predicted = semantics_predicted * mask_int + semantics * (1-mask_int)\n",
        "    semantics_predicted_sparse = label_to_sparse(semantics_predicted)\n",
        "    IoU = getIoU(semantics, semantics_predicted, locations)\n",
        "    \n",
        "    prediction = generatorSEMGAN([masked_image, semantics_predicted_sparse], training=True)\n",
        "    prediction = prediction * mask + image*(1-mask)\n",
        "    \n",
        "    target_path_im = os.path.join(path, \"SemGAN\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "    target_path_sem = os.path.join(path, \"SemGAN\", \"semantics\", str(step) + \".png\")\n",
        "    semantics_predicted = label_to_rgb(semantics_predicted)\n",
        "    tf.keras.utils.save_img(target_path_sem, semantics_predicted[0])\n",
        "\n",
        "    metrics_list_SEMGAN.append(splitMaskMetrics(image, prediction, locations))\n",
        "    IoU_list.append(IoU)\n",
        "\n",
        "    semantics_sparse = label_to_sparse(semantics)\n",
        "    prediction = generatorSEMGAN_GT([masked_image, semantics_sparse], training=True) #estimated values for each of 35 classes\n",
        "    prediction = prediction * mask + image*(1-mask) #insert generated label patches to inverted ground truth\n",
        "    target_path_im = os.path.join(path, \"SemGAN_GT\", str(step) + \".png\")\n",
        "    tf.keras.utils.save_img(target_path_im, prediction[0])\n",
        "\n",
        "    target_path_sem = os.path.join(path, \"SemGAN_GT\", \"semantics\", str(step) + \".png\")\n",
        "    semantics = label_to_rgb(semantics)\n",
        "    tf.keras.utils.save_img(target_path_sem, semantics[0])\n",
        "                            \n",
        "    metrics_list_SEMGAN_GT.append(splitMaskMetrics(image, prediction, locations))\n",
        "\n",
        "    print(step)\n",
        "    \n",
        "  \n",
        "  metrics_df_GAN = pd.DataFrame(metrics_list_GAN)\n",
        "  metrics_df_GAN.columns = names\n",
        "  metrics_df_GAN[\"GT\"] = GT_list\n",
        "  metrics_df_GAN[\"Mask\"] = mask_list\n",
        "  metrics_df_GAN[\"Locations\"]=location_list\n",
        "\n",
        "  metrics_df_SEMGAN = pd.DataFrame(metrics_list_SEMGAN)\n",
        "  metrics_df_SEMGAN.columns = names\n",
        "  metrics_df_SEMGAN[\"IoU\"] = IoU_list\n",
        "\n",
        "  metrics_df_SEMGAN_GT = pd.DataFrame(metrics_list_SEMGAN_GT)\n",
        "  metrics_df_SEMGAN_GT.columns = names\n",
        "\n",
        "  return metrics_df_GAN, metrics_df_SEMGAN, metrics_df_SEMGAN_GT"
      ],
      "metadata": {
        "id": "U61XVdExgKbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for datas in tf_val.shuffle(10).take(1):\n",
        "  image, masked_image, semantics, semantics_masked, mask, locations = datas"
      ],
      "metadata": {
        "id": "XWZWRmGIno8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantics_predicted = gen_to_label(generator_semantics(semantics_masked, training = True))\n",
        "mask_int =  tf.cast(mask, tf.int32)\n",
        "semantics_predicted = semantics_predicted * mask_int + semantics * (1-mask_int)\n",
        "semantics_predicted_sparse = label_to_sparse(semantics_predicted)\n",
        "\n",
        "prediction = generatorSEMGAN([masked_image, semantics_predicted_sparse], training=True)\n",
        "prediction = prediction * mask + image*(1-mask)\n",
        "plt.imshow(prediction[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "EM9b05BxniNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantics_sparse = label_to_sparse(semantics)\n",
        "prediction_GT = generatorSEMGAN_GT([masked_image, semantics_sparse], training=True) #estimated values for each of 35 classes\n",
        "prediction_GT = prediction_GT * mask + image*(1-mask) #insert generated label patches to inverted ground truth\n",
        "plt.imshow(prediction_GT[0]*0.5+0.5)"
      ],
      "metadata": {
        "id": "SQbV22YSnnkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/predictions\n",
        "! mkdir /content/predictions/GAN\n",
        "! mkdir /content/predictions/SemGAN\n",
        "! mkdir /content/predictions/SemGAN_GT\n",
        "! mkdir /content/predictions/SemGAN/semantics\n",
        "! mkdir /content/predictions/SemGAN_GT/semantics"
      ],
      "metadata": {
        "id": "bhHzqTVUnCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_GAN, metrics_ft_SEMGAN, metrics_ft_SEMGAN_GT = get_dataset_metrics(tf_val)"
      ],
      "metadata": {
        "id": "Hi6LG8u_owHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GAN\")\n",
        "print(\"Mean: \", metrics_ft_GAN[\"L1\"].mean(), \"Std: \", metrics_ft_GAN[\"L1\"].std())\n",
        "print(\"Mean: \", metrics_ft_GAN[\"PSNR\"].mean(), \"Std: \", metrics_ft_GAN[\"PSNR\"].std())\n",
        "print(\"Mean: \", metrics_ft_GAN[\"SSIM\"].mean(), \"Std: \", metrics_ft_GAN[\"SSIM\"].std())\n",
        "print(\"SemGAN\")\n",
        "print(\"Mean: \", metrics_ft_SEMGAN[\"L1\"].mean(), \"Std: \", metrics_ft_SEMGAN[\"L1\"].std())\n",
        "print(\"Mean: \", metrics_ft_SEMGAN[\"PSNR\"].mean(), \"Std: \", metrics_ft_SEMGAN[\"PSNR\"].std())\n",
        "print(\"Mean: \", metrics_ft_SEMGAN[\"SSIM\"].mean(), \"Std: \", metrics_ft_SEMGAN[\"SSIM\"].std())\n",
        "print(\"SemGAN-GT\")\n",
        "print(\"Mean: \", metrics_ft_SEMGAN_GT[\"L1\"].mean(), \"Std: \", metrics_ft_SEMGAN_GT[\"L1\"].std())\n",
        "print(\"Mean: \", metrics_ft_SEMGAN_GT[\"PSNR\"].mean(), \"Std: \", metrics_ft_SEMGAN_GT[\"PSNR\"].std())\n",
        "print(\"Mean: \", metrics_ft_SEMGAN_GT[\"SSIM\"].mean(), \"Std: \", metrics_ft_SEMGAN_GT[\"SSIM\"].std())"
      ],
      "metadata": {
        "id": "qoMdFrR1-Dru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 120\n",
        "print(metrics_ft_GAN[[\"L1\", \"PSNR\", \"SSIM\"]].iloc[idx])\n",
        "print(metrics_ft_SEMGAN[[\"L1\", \"PSNR\", \"SSIM\"]].iloc[idx])\n",
        "print(metrics_ft_SEMGAN_GT[[\"L1\", \"PSNR\", \"SSIM\"]].iloc[idx])"
      ],
      "metadata": {
        "id": "ix36YnrmKsrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN"
      ],
      "metadata": {
        "id": "L6vRUciIsQm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_ft_SEMGAN_GT"
      ],
      "metadata": {
        "id": "k33dUKNDsR1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = \"PSNR\"\n",
        "for i in range(len(metrics_ft_SEMGAN_GT)):\n",
        "  GAN_score = metrics_ft_GAN[score].iloc[i]\n",
        "  SemGAN_score = metrics_ft_SEMGAN_GT[score].iloc[i]\n",
        "  if GAN_score - SemGAN_score > 2:\n",
        "    print(f\"Image {i}, pix2pix has a score of: {GAN_score} and SemGAN has a score of: {SemGAN_score}\")"
      ],
      "metadata": {
        "id": "qgm4ApreYRLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the images"
      ],
      "metadata": {
        "id": "4zo8IZXvglYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_location(image, location, size):\n",
        "  im_len = len(image.shape)\n",
        "  width, height = size\n",
        "  width = width/2\n",
        "  height = height/2\n",
        "\n",
        "  mask_height = tf.cast(location[0]/2, tf.int32).numpy()\n",
        "  mask_width = tf.cast(location[1]/2, tf.int32).numpy()\n",
        "  height_offset = location[2]\n",
        "  width_offset = location[3]\n",
        "\n",
        "  mask_y_begin = height_offset-mask_height\n",
        "  mask_y_end = height_offset+mask_height\n",
        "  mask_x_begin = width_offset-mask_width\n",
        "  mask_x_end = width_offset+mask_width\n",
        "\n",
        "  mask_y_begin = mask_y_begin-height if mask_y_begin-height >= 0 else 0\n",
        "  mask_y_end = mask_y_end + height if mask_y_end+ height <= 256 else 256\n",
        "  mask_x_begin = mask_x_begin-width if mask_x_begin-width >= 0 else 0\n",
        "  mask_x_end = mask_x_end+width if mask_x_end+width <= 512 else 512\n",
        "\n",
        "  mask_y_begin = int(mask_y_begin)\n",
        "  mask_y_end = int(mask_y_end)\n",
        "  mask_x_begin = int(mask_x_begin)\n",
        "  mask_x_end = int(mask_x_end)\n",
        "\n",
        "\n",
        "  targetMask = image[:, mask_y_begin:mask_y_end,mask_x_begin:mask_x_end,:] if im_len == 4 else image[mask_y_begin:mask_y_end,\n",
        "                                                                                                     mask_x_begin:mask_x_end,:]\n",
        "\n",
        "  return targetMask"
      ],
      "metadata": {
        "id": "6uvRXcQ7QpW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_rows(row):\n",
        "  path = \"/content/predictions\"\n",
        "  GT = metrics_ft_GAN.iloc[row][\"GT\"]\n",
        "  mask = metrics_ft_GAN.iloc[row][\"Mask\"]\n",
        "  inp_im = tf.where(mask == 1, -1, GT)\n",
        "  locations = metrics_ft_GAN.iloc[row][\"Locations\"]\n",
        "  locations = np.squeeze(locations.numpy(), axis = 0)\n",
        "  patch_size = (50,50)\n",
        "  \n",
        "\n",
        "  GAN_pred = normalize(load(os.path.join(path, \"GAN\", str(row)+\".png\"), 3))\n",
        "  SEMGAN_pred = normalize(load(os.path.join(path, \"SemGAN\", str(row)+\".png\"), 3))\n",
        "  SEMGAN_GT_pred = normalize(load(os.path.join(path, \"SemGAN_GT\", str(row)+\".png\"), 3))\n",
        "\n",
        "  #semantics_pred = load(os.path.join(path, \"SemGAN\", \"semantics\", str(row)+\".png\"), 3)\n",
        "  #semantics_pred = normalize(semantics_pred)\n",
        "  #semantics = load(os.path.join(path, \"SemGAN_GT\", \"semantics\", str(row)+\".png\"), 3)\n",
        "\n",
        "  fix, axs = plt.subplots(4,5, figsize = (20,20))\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  \n",
        "  images = [inp_im[0], GT[0], GAN_pred, SEMGAN_pred, SEMGAN_GT_pred]\n",
        "  for i,location in enumerate(locations):\n",
        "    for j, im in enumerate(images):\n",
        "      patch = get_location(im, location, patch_size)\n",
        "      axs[i][j].imshow(patch*0.5+0.5)\n",
        "      axs[i][j].axis(\"off\")\n",
        "\n"
      ],
      "metadata": {
        "id": "M3M1Nx_btBow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics_ft_SEMGAN.sort_values(by=['SSIM'], ascending=True).head(10))"
      ],
      "metadata": {
        "id": "n5-RZlP5Hi1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_rows(288)"
      ],
      "metadata": {
        "id": "CU6w0Yv24Vkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_images():\n",
        "  path = \"/content/predictions\"\n",
        "  "
      ],
      "metadata": {
        "id": "iIsugQ1deWja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pd_visualize_rows(ds, rows):\n",
        "  n = len(rows)\n",
        "  fix, axs = plt.subplots(n,3, figsize = (20,20))\n",
        "  plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "  for i, row in enumerate(rows):\n",
        "    l1, ssim, psnr, image, semantics, mask = ds.iloc[row]\n",
        "    masked_image = tf.where(mask == 1, -1, image)\n",
        "    prediction = generator(masked_image, training = True)\n",
        "    prediction = prediction * mask + image * (1-mask)\n",
        "\n",
        "    if n > 1:\n",
        "      axs[i][0].imshow(denormalize(image[0]))\n",
        "      axs[i][1].imshow(denormalize(masked_image[0]))\n",
        "      axs[i][2].imshow(denormalize(prediction[0]))\n",
        "      for ax in axs[i]:\n",
        "          ax.axis(\"off\")\n",
        "    else:\n",
        "      axs[0].imshow(denormalize(image[0]))\n",
        "      axs[1].imshow(denormalize(masked_image[0]))\n",
        "      axs[2].imshow(denormalize(prediction[0]))\n",
        "\n",
        "      for ax in axs:\n",
        "          ax.axis(\"off\")\n",
        "      \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "N1Etht3_gk0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}